{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44644603",
   "metadata": {},
   "source": [
    "# üéØ | Cora-For-Zava: Advanced Model Distillation\n",
    "\n",
    "Welcome! This notebook demonstrates how to use model distillation to transfer specialized knowledge from a large, expensive model to a smaller, faster, and more cost-effective one while maintaining quality standards.\n",
    "\n",
    "## üõí Our Zava Scenario\n",
    "\n",
    "**Cora** is a customer service chatbot for **Zava** - a fictitious retailer of home improvement goods for DIY enthusiasts. You've fine-tuned a model for excellent tone and style, but it's using a large, expensive model (GPT-4) for a focused task. This notebook shows how to distill that specialized knowledge into a smaller model (GPT-4.1-nano) for optimal cost-efficiency and performance.\n",
    "\n",
    "## üéØ What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ‚úÖ Configured distillation pipeline using Azure AI evaluation and fine-tuning\n",
    "- ‚úÖ Used your custom grader to measure baseline model performance  \n",
    "- ‚úÖ Applied distillation techniques to transfer knowledge to a smaller model\n",
    "- ‚úÖ Validated that the distilled model maintains quality standards\n",
    "- ‚úÖ Measured performance improvements in cost, speed, and efficiency\n",
    "- ‚úÖ Created a production-ready optimized model deployment\n",
    "\n",
    "## üí° What You'll Learn\n",
    "\n",
    "- How to implement end-to-end model distillation workflows\n",
    "- How to use Azure AI Foundry's evaluation and fine-tuning services together\n",
    "- How to balance model capabilities with cost and performance requirements\n",
    "- How to validate distillation effectiveness using custom graders\n",
    "- How to measure and optimize model efficiency without sacrificing quality\n",
    "- When distillation provides the best ROI vs. other optimization approaches\n",
    "\n",
    "> **Key Concept**: Model distillation enables \"teaching\" smaller models to perform specialized tasks by learning from larger, more capable \"teacher\" models, achieving dramatic cost and performance improvements.\n",
    "\n",
    "Ready to optimize your AI for maximum efficiency? Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd8483",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment Variables\n",
    "\n",
    "The following environment variables should already be configured in your `.env` file from the earlier setup steps:\n",
    "\n",
    "- **AZURE_OPENAI_API_KEY**: Your Azure OpenAI API key\n",
    "- **AZURE_OPENAI_ENDPOINT**: Your Azure OpenAI service endpoint\n",
    "- **AZURE_OPENAI_API_VERSION**: The API version (2025-02-01-preview for distillation features)\n",
    "- **AZURE_SUBSCRIPTION_ID**: Your Azure subscription ID\n",
    "- **AZURE_RESOURCE_GROUP**: Your Azure resource group name\n",
    "- **AZURE_AI_PROJECT_NAME**: Your Azure AI Foundry project name\n",
    "\n",
    "> **Important**: Distillation requires access to both teacher models (large, capable) and student models (small, efficient). This notebook uses GPT-4 as the teacher and GPT-4.1-nano as the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56955b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure required environment variables are set\n",
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "model_name = \"gpt-4.1\"\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-02-01-preview\")\n",
    "\n",
    "if not openai_key or not openai_endpoint:\n",
    "    print(\"Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "print(\"Using Model:\", model_name)\n",
    "print(\"Using API Version:\", api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465864fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create an Azure OpenAI Client instance\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-04-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ea17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And create a handy UUID we can use to track each run of this notebook\n",
    "import uuid\n",
    "UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5478eb",
   "metadata": {},
   "source": [
    "## Step 2: Execute Knowledge Distillation\n",
    "\n",
    "Knowledge distillation transfers specialized capabilities from a \"teacher\" model to a \"student\" model. This process enables smaller models to achieve performance similar to larger ones on specific tasks.\n",
    "\n",
    "### üß† Distillation Process Overview\n",
    "\n",
    "**Teacher Model** ‚Üí **Knowledge Transfer** ‚Üí **Student Model**\n",
    "- Large, expensive, capable ‚Üí Specialized training data ‚Üí Small, fast, cost-effective\n",
    "\n",
    "**Key Benefits**:\n",
    "- **Cost Reduction**: Smaller models use fewer tokens per inference\n",
    "- **Performance Boost**: Faster response times and higher throughput\n",
    "- **Quality Preservation**: Maintains specialized task performance\n",
    "- **Scalability**: Enables deployment in resource-constrained environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8a981",
   "metadata": {},
   "source": [
    "### Step 2.1: Identify the Best Teacher Model\n",
    "\n",
    "Load evaluation results from your custom grader to identify the highest-performing model. This \"winning\" model becomes our teacher for the distillation process.\n",
    "\n",
    "**Selection Criteria**:\n",
    "- **Quality Scores**: Highest grader ratings for tone and style\n",
    "- **Consistency**: Reliable performance across diverse customer scenarios\n",
    "- **Zava Standards**: Best adherence to polite, helpful, factual response patterns\n",
    "\n",
    "> **Data Source**: Using exported results from the previous custom grader notebook (`32-distillation_export.json`) to ensure continuity in our evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea22fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load exported data from grader notebook\n",
    "with open('32-distillation_export.json', 'r') as f:\n",
    "    export_data = json.load(f)\n",
    "\n",
    "# Reconstruct the baseline runs\n",
    "baseline_eval_id = export_data['baseline_eval_id']\n",
    "baseline_runs = []\n",
    "\n",
    "for i, run_id in enumerate(export_data['baseline_runs_ids']):\n",
    "    run = client.evals.runs.retrieve(eval_id=baseline_eval_id, run_id=run_id)\n",
    "    run.model = export_data['baseline_runs_models'][i]  # Ensure model name is set\n",
    "    baseline_runs.append(run)\n",
    "\n",
    "# Get other variables\n",
    "baseline_eval = client.evals.retrieve(baseline_eval_id)\n",
    "qa_validation = export_data['qa_validation']\n",
    "GRADER_MODEL = export_data['GRADER_MODEL']\n",
    "GRADER_PROMPT = export_data['GRADER_PROMPT'] \n",
    "SYSTEM_PROMPT = export_data['SYSTEM_PROMPT']\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(baseline_runs)} baseline runs from grader notebook\")\n",
    "print(f\"‚úÖ Loaded {len(qa_validation)} validation Q&A pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2bb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite metric based teacher/student selection\n",
    "# Uses existing score_distribution + HIGH_SCORES already populated above\n",
    "\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "CUTOFF = 4.0\n",
    "HIGH_SCORES = {\n",
    "    \"o3-mini\": [],\n",
    "    \"o4-mini\": [],\n",
    "    \"gpt-4o-mini\": [],\n",
    "    \"gpt-4.1-mini\": [],\n",
    "    \"gpt-4.1-nano\": [],\n",
    "}\n",
    "\n",
    "# Let's find our responses that were Excellent (at or above CUTOFF). We'll collect them\n",
    "# and pre-format them into chat completions format to save time later.\n",
    "#\n",
    "# This part is honestly a bit tricky...we're extracting the prompts and responses for the\n",
    "# model under test and *not* the prompts to the grader, so we have to do surgery. üî™\n",
    "for run in baseline_runs:\n",
    "    pages = client.evals.runs.output_items.list(run.id, eval_id=baseline_eval.id).iter_pages()\n",
    "    for page in pages:\n",
    "        for item in page.data:\n",
    "            # We only used 1 grader. If you use multiple, you should look for which ones you want.\n",
    "            if not item.results:\n",
    "                continue\n",
    "            result = item.results[0]\n",
    "            # FIX: Use attribute access for score and sample\n",
    "            if result.score >= CUTOFF:\n",
    "                generated = result.sample[\"input\"][-1][\"content\"].strip().split(\"\\nA: \")\n",
    "                question = generated[0][3:] # drops the \"Q: \"\n",
    "                answer = generated[-1]\n",
    "                messages = [\n",
    "                    { \"role\": \"system\", \"content\": SYSTEM_PROMPT },\n",
    "                    { \"role\": \"user\", \"content\": question },\n",
    "                    { \"role\": \"assistant\", \"content\": answer },\n",
    "                ]\n",
    "                HIGH_SCORES[run.model].append({ \"messages\": messages })\n",
    "\n",
    "# 1. Collect raw score lists for each model (recompute if needed)\n",
    "score_distribution = {}\n",
    "for run in baseline_runs:\n",
    "    score_distribution.setdefault(run.model, [])\n",
    "    pages = client.evals.runs.output_items.list(run.id, eval_id=baseline_eval.id).iter_pages()\n",
    "    for page in pages:\n",
    "        for item in page.data:\n",
    "            if not item.results:\n",
    "                continue\n",
    "            result = item.results[0]\n",
    "            # FIX: Use attribute access for score\n",
    "            if hasattr(result, \"score\"):\n",
    "                score_distribution[run.model].append(result.score)\n",
    "\n",
    "# 2. Define model size ordering (left = larger / more capable)\n",
    "SIZE_ORDER = [\"o3-mini\", \"gpt-4.1\", \"o4-mini\", \"gpt-4o-mini\", \"gpt-4.1-nano\"]\n",
    "SIZE_INDEX = {m:i for i,m in enumerate(SIZE_ORDER)}\n",
    "\n",
    "# 3. Composite quality metric\n",
    "#    quality = 0.5*mean + 0.3*p90 + 0.2*coverage  (coverage scaled to 0-10)\n",
    "#    p90 stabilizes performance tail; coverage rewards consistency\n",
    "\n",
    "def composite_quality(scores, cutoff):\n",
    "    if not scores:\n",
    "        return -1\n",
    "    scores_sorted = sorted(scores)\n",
    "    p90 = scores_sorted[math.floor(0.9*(len(scores_sorted)-1))]\n",
    "    coverage = sum(1 for s in scores if s >= cutoff) / len(scores) if scores else 0.0\n",
    "    return 0.5*mean(scores) + 0.3*p90 + 0.2*(coverage*10)  # coverage scaled\n",
    "\n",
    "# 4. Pick teacher = argmax composite_quality (ties broken by larger model first)\n",
    "cutoff = CUTOFF  # reuse existing threshold\n",
    "model_metrics = []\n",
    "for model, scores in score_distribution.items():\n",
    "    cq = composite_quality(scores, cutoff)\n",
    "    model_metrics.append({\n",
    "        \"model\": model,\n",
    "        \"mean\": round(mean(scores),2) if scores else 0,\n",
    "        \"p90\": round(sorted(scores)[math.floor(0.9*(len(scores)-1))],2) if scores else 0,\n",
    "        \"coverage\": round(sum(1 for s in scores if s >= cutoff)/len(scores),2) if scores else 0,\n",
    "        \"high_scores\": len([s for s in scores if s >= cutoff]),\n",
    "        \"n\": len(scores),\n",
    "        \"composite\": round(cq,3),\n",
    "        \"size_index\": SIZE_INDEX.get(model, 999)\n",
    "    })\n",
    "\n",
    "# Sort for display\n",
    "model_metrics.sort(key=lambda d: d['composite'], reverse=True)\n",
    "TEACHER_MODEL = model_metrics[0]['model'] if model_metrics else None\n",
    "\n",
    "# 5. Student selection: choose smallest model with >= MIN_STUDENT_SAMPLES high scores\n",
    "MIN_STUDENT_SAMPLES = 10\n",
    "\n",
    "eligible_students = []\n",
    "for m in HIGH_SCORES:\n",
    "    if m == TEACHER_MODEL:\n",
    "        continue\n",
    "    if m not in SIZE_INDEX or TEACHER_MODEL not in SIZE_INDEX:\n",
    "        continue\n",
    "    if SIZE_INDEX[m] <= SIZE_INDEX[TEACHER_MODEL]:  # must be strictly smaller\n",
    "        continue\n",
    "    high_count = len(HIGH_SCORES[m])\n",
    "    if high_count >= MIN_STUDENT_SAMPLES:\n",
    "        eligible_students.append((m, high_count))\n",
    "\n",
    "if eligible_students:\n",
    "    # Prefer smallest (highest size index), tie-break by more high samples\n",
    "    eligible_students.sort(key=lambda t: (SIZE_INDEX[t[0]], -t[1]))\n",
    "    STUDENT_MODEL = eligible_students[-1][0]\n",
    "else:\n",
    "    # Relax: pick smallest model with ANY high scores, else default nano\n",
    "    relaxed = [(m, len(HIGH_SCORES[m])) for m in HIGH_SCORES \n",
    "               if m != TEACHER_MODEL and m in SIZE_INDEX and SIZE_INDEX[m] > SIZE_INDEX[TEACHER_MODEL] and len(HIGH_SCORES[m]) > 0]\n",
    "    if relaxed:\n",
    "        relaxed.sort(key=lambda t: (SIZE_INDEX[t[0]], -t[1]))\n",
    "        STUDENT_MODEL = relaxed[-1][0]\n",
    "    else:\n",
    "        STUDENT_MODEL = \"gpt-4.1-nano\"\n",
    "\n",
    "print(\"=== Model Composite Metrics ===\")\n",
    "for mm in model_metrics:\n",
    "    star = \"<- TEACHER\" if mm['model'] == TEACHER_MODEL else \"\"\n",
    "    print(f\"{mm['model']:12s} comp={mm['composite']:5.2f} mean={mm['mean']:4.2f} p90={mm['p90']:4.2f} cov={mm['coverage']:4.2f} hi={mm['high_scores']:3d}/{mm['n']:3d} {star}\")\n",
    "\n",
    "print(f\"\\nSelected Teacher: {TEACHER_MODEL}\")\n",
    "print(f\"Selected Student: {STUDENT_MODEL} (high samples: {len(HIGH_SCORES[STUDENT_MODEL])})\")\n",
    "\n",
    "# 6. (Optional) Raise cutoff adaptively if too many highs (all models >90% coverage)\n",
    "if all(mm['coverage'] > 0.9 for mm in model_metrics if mm['n'] > 0):\n",
    "    print(\"‚ö†Ô∏è Coverage very high across all models ‚Äî consider increasing CUTOFF to better separate quality.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6fef4",
   "metadata": {},
   "source": [
    "### Step 2.2: Generate Distillation Training Data\n",
    "\n",
    "Create high-quality training datasets from the teacher model's best responses. These examples will teach the student model to replicate the teacher's specialized knowledge and behavior patterns.\n",
    "\n",
    "**Data Preparation**:\n",
    "- **Source**: High-scoring responses from teacher model evaluation\n",
    "- **Split**: 80% training / 20% validation for optimal learning\n",
    "- **Format**: JSONL format compatible with Azure OpenAI fine-tuning\n",
    "- **Quality**: Only responses that scored highly on our custom grader criteria\n",
    "\n",
    "**Distillation Strategy**: Use the teacher's best outputs as training examples to embed Zava's tone and style directly into the smaller student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38531244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we go any further, let's turn our collected excellent responses into our\n",
    "# training and validation fine-tuning datasets. Like before, we have to write these\n",
    "# to disk and then upload them via the Files API.\n",
    "training_filename = f\"zava-tone-training-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "validation_filename = f\"zava-tone-validation-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "# Make an 80/20 split to form our training/validation data.\n",
    "split_at = int(len(HIGH_SCORES[TEACHER_MODEL]) * 0.80)\n",
    "training_data = HIGH_SCORES[TEACHER_MODEL][:split_at]\n",
    "validation_data = HIGH_SCORES[TEACHER_MODEL][split_at:]\n",
    "print(f\"Split into {len(training_data)} training / {len(validation_data)} validation rows.\")\n",
    "\n",
    "# Create and upload the training data.\n",
    "with open(training_filename, \"w\") as f:\n",
    "    for message in training_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")\n",
    "with open(training_filename, \"rb\") as f:\n",
    "    training_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    training_file = client.files.wait_for_processing(training_file.id)\n",
    "print(f\"üèãÔ∏è‚Äç‚ôÇÔ∏è Created training file:\\n{training_file.to_json(indent=2)}\")\n",
    "\n",
    "# Create and upload the validation data.\n",
    "with open(validation_filename, \"w\") as f:\n",
    "    for message in validation_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")\n",
    "with open(validation_filename, \"rb\") as f:\n",
    "    validation_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "    validation_file = client.files.wait_for_processing(validation_file.id)\n",
    "print(f\"üìã Created validation file:\\n{validation_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13a2fb",
   "metadata": {},
   "source": [
    "### Step 2.3: Train the Student Model\n",
    "\n",
    "Submit a fine-tuning job to transfer knowledge from teacher to student. The student model (GPT-4.1-nano) will learn to replicate the teacher's specialized Zava customer service patterns.\n",
    "\n",
    "**Training Configuration**:\n",
    "- **Student Model**: GPT-4.1-nano (smaller, faster, more cost-effective)\n",
    "- **Training Data**: High-quality examples from the teacher model\n",
    "- **Validation Monitoring**: Real-time loss tracking for training optimization\n",
    "- **Hyperparameters**: Optimized settings for faster convergence\n",
    "\n",
    "**Expected Outcome**: A smaller model that maintains the teacher's quality standards while providing significant cost and performance benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c79880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start training! Submit our fine-tuning job to teach 4.1-nano new tricks.\n",
    "## NN-TOFIX: Check STUDENT MODEL name\n",
    "TEACHER_MODEL = TEACHER_MODEL\n",
    "STUDENT_MODEL = STUDENT_MODEL\n",
    "SUFFIX = f\"{TEACHER_MODEL}-zava-tone-{UNIQUE_ENOUGH_KEY}\".replace(\".\", \"\") # '.' is a reserved character üòú\n",
    "\n",
    "# OPTIMIZATION 4: Optimized hyperparameters for faster convergence\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    model=STUDENT_MODEL,\n",
    "    suffix=SUFFIX,\n",
    "    training_file=training_file.id,\n",
    "    validation_file=validation_file.id,\n",
    "    extra_body={ \"trainingType\": \"globalstandard\" },\n",
    ")\n",
    "print(f\"üë®‚Äçüî¨ Created fine-tuning job:\\n{job.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fc867",
   "metadata": {},
   "source": [
    "### Step 2.4: Monitor Training Progress\n",
    "\n",
    "Track fine-tuning job progress with real-time updates and early stopping capabilities. The training includes optimizations for faster convergence and better resource utilization.\n",
    "\n",
    "**Monitoring Features**:\n",
    "- **Progress Tracking**: Real-time status updates every 10 seconds\n",
    "- **Early Stopping**: Automatic halt when training plateaus\n",
    "- **Validation Loss**: Continuous monitoring to prevent overfitting\n",
    "- **Time Management**: Elapsed time tracking for performance optimization\n",
    "\n",
    "> **Training Time**: Typically 10-30 minutes depending on dataset size and model complexity. The system includes patience controls to avoid unnecessary training when improvement stalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd907ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION 1: Add early stopping and progress monitoring to speed up training\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "last_loss = None\n",
    "patience_counter = 0\n",
    "PATIENCE = 3  # Stop if no improvement for 3 checks\n",
    "\n",
    "status = job.status\n",
    "while status not in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "    time.sleep(10)\n",
    "    job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "    status = job.status\n",
    "    \n",
    "    # OPTIMIZATION 2: Monitor training metrics for early stopping\n",
    "    try:\n",
    "        events = client.fine_tuning.jobs.list_events(job.id, limit=5)\n",
    "        if events.data:\n",
    "            latest_event = events.data[0]\n",
    "            if hasattr(latest_event, 'data') and 'train_loss' in latest_event.data:\n",
    "                current_loss = latest_event.data['train_loss']\n",
    "                if last_loss and current_loss >= last_loss:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= PATIENCE:\n",
    "                        print(f\"‚ö†Ô∏è Early stopping: Loss plateau detected\")\n",
    "                        # Could implement job cancellation here if needed\n",
    "                else:\n",
    "                    patience_counter = 0\n",
    "                last_loss = current_loss\n",
    "                print(f\"üìà Latest train_loss: {current_loss:.4f}\")\n",
    "    except Exception as e:\n",
    "        pass  # Continue without early stopping if events unavailable\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"üë®‚Äçüî¨ Job {job.id}: {status}\")\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "if status == \"succeeded\":\n",
    "    print(f\"üèÅ Fine-tuning finished!\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Fine-tuning job did not complete successfully (status={status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d108c2",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate Distilled Model Performance\n",
    "\n",
    "Validate that knowledge distillation was successful by testing the student model against quality standards and comparing it to the original teacher model.\n",
    "\n",
    "### üéØ Evaluation Strategy\n",
    "\n",
    "**Multi-Dimensional Assessment**:\n",
    "1. **Quality Preservation**: Use the same custom grader to ensure consistency\n",
    "2. **Performance Comparison**: Measure student vs. teacher on identical test data\n",
    "3. **Cost-Benefit Analysis**: Quantify improvements in speed and cost efficiency\n",
    "4. **Generalization Testing**: Verify the model maintains broad capabilities\n",
    "\n",
    "**Success Criteria**: Student model should match teacher quality while providing measurable cost and performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224127cf",
   "metadata": {},
   "source": [
    "### Step 3.1: Deploy the Distilled Student Model\n",
    "\n",
    "Deploy the fine-tuned student model for testing and evaluation. Using Developer Tier keeps costs minimal while allowing comprehensive quality assessment.\n",
    "\n",
    "**Deployment Configuration**:\n",
    "- **Tier**: Developer (cost-effective for testing and evaluation)\n",
    "- **Model**: Fine-tuned student model from distillation training\n",
    "- **Purpose**: Quality validation and performance benchmarking\n",
    "- **Duration**: Temporary deployment for evaluation (auto-cleanup after 24 hours)\n",
    "\n",
    "> **Cost Management**: Developer tier provides full functionality without hosting fees, perfect for validating distillation results before production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to deploy our fine-tuned model. We'll use Developer Tier to keep\n",
    "# costs under control for evaluation.\n",
    "\n",
    "# We can't do this with the OpenAI SDK, so we need to reach for the Azure SDK.\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "\n",
    "cogsvc_client = CognitiveServicesManagementClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    ")\n",
    "\n",
    "# Define our Deployment. Note the use of SKU for specificy capacity and\n",
    "# the name of the deployment tier.\n",
    "DEPLOYMENT_NAME = f\"zava-tone-distilled-{SUFFIX}\"\n",
    "DEPLOYMENT = {\n",
    "    \"properties\": {\n",
    "        \"model\": { \n",
    "            \"format\": \"OpenAI\", \n",
    "            \"name\": job.fine_tuned_model, \n",
    "            \"version\": \"1\" \n",
    "        },\n",
    "    },\n",
    "    \"sku\": { \n",
    "        \"capacity\": 250, \n",
    "        \"name\": \"DeveloperTier\" \n",
    "    },\n",
    "}\n",
    "\n",
    "# Submit the request for provisioning. This may take a few minutes, so we\n",
    "# poll for updates. If it already exists, this should return quickly. Since\n",
    "# we're deploying a 4.1-nano model, this should only take 3-5 minutes tops.\n",
    "deployment = cogsvc_client.deployments.begin_create_or_update(\n",
    "    resource_group_name=os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    account_name=os.environ.get(\"AZURE_AI_FOUNDRY_NAME\"),\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    deployment=DEPLOYMENT,\n",
    ")\n",
    "print(f\"üõ≥Ô∏è Submitted deployment {deployment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d950f0",
   "metadata": {},
   "source": [
    "### 3.2 Wait Till Deployment Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88230a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for our deployment to finish provisioning.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "status = deployment.status()\n",
    "\n",
    "while status not in [\"Succeeded\", \"Failed\"]:\n",
    "    deployment.wait(5)\n",
    "    status = deployment.status()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"üõ≥Ô∏è Provisioning {DEPLOYMENT_NAME}: {status}\")\n",
    "    print(\"‚è±Ô∏èElapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ Provisioning finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83dd8a",
   "metadata": {},
   "source": [
    "### Step 3.3: Prepare Validation Dataset\n",
    "\n",
    "Upload the validation dataset for comprehensive evaluation of the distilled student model. This data will be used to compare student performance against teacher model baselines.\n",
    "\n",
    "**Validation Setup**:\n",
    "- **Dataset**: Same questions used for teacher model evaluation\n",
    "- **Format**: JSONL format compatible with Azure AI evaluation services\n",
    "- **Purpose**: Direct comparison between teacher and student model outputs\n",
    "- **Consistency**: Ensures fair evaluation using identical test conditions\n",
    "\n",
    "> **Evaluation Strategy**: Using the same validation data allows precise measurement of knowledge transfer effectiveness and quality preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll upload our post-training validation dataset and prepare our final Evaluation.\n",
    "# We need to save the data to disk first, again for...reasons.\n",
    "filename = f\"./zava-tone-posttraining-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for row in qa_validation:\n",
    "        json.dump(row, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "posttraining_file = None\n",
    "with open(filename, \"rb\") as f:\n",
    "    posttraining_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    posttraining_file = client.files.wait_for_processing(posttraining_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216611d",
   "metadata": {},
   "source": [
    "### Step 3.4: Execute Comprehensive Model Comparison\n",
    "\n",
    "Run side-by-side evaluation of the student model against teacher and peer models using our custom grader. This validates distillation effectiveness and measures quality preservation.\n",
    "\n",
    "**Comparison Framework**:\n",
    "- **Student Model**: Fine-tuned GPT-4.1-nano (distilled)\n",
    "- **Teacher Model**: Original high-performing model used for distillation\n",
    "- **Peer Models**: Other models for baseline comparison\n",
    "- **Evaluation Method**: Same custom grader used throughout the pipeline\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Quality Scores**: Polite, helpful, factual response ratings\n",
    "- **Consistency**: Response pattern adherence to Zava standards\n",
    "- **Performance**: Speed and cost efficiency measurements\n",
    "- **Coverage**: Ability to handle diverse customer scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05976b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our Evaluation for validating our choice in grader prompt and model.\n",
    "\n",
    "# The entire user prompt is data driven from the file. No generation is done using\n",
    "# a model in this case, just simple string substitution using this pattern. This\n",
    "# means we directly reference the two fields in our baseline.jsonl file.\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{item.answer}}\n",
    "\"\"\"\n",
    "\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# We need to describe what our evaluation dataset looks like.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    }\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA,\n",
    "    \"include_sample_schema\": False,\n",
    "    \"type\": \"custom\",\n",
    "}\n",
    "\n",
    "# Lastly, we define test criteria that combines all the above.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Zava Tone Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],    # Our grader scores in a range from 1 to 10\n",
    "    \"pass_threshold\": 4.0,   # Let's say a 4 is \"passing\" for now.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ac102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a final Eval using our post-training dataset that doesn't overlap with the\n",
    "# original distillation and training dataset. This lets us judge our new model based on\n",
    "# data it hasn't seen before. We'll also through in one of our better performing base\n",
    "# models as a control.\n",
    "POST_EVAL_MODELS = [\n",
    "    DEPLOYMENT_NAME,# distilled\n",
    "    \"gpt-4.1-nano\", # student\n",
    "    \"gpt-4.1\",      # control\n",
    "]\n",
    "\n",
    "# SCHEMA, GRADER_MODEL, and INPUT are re-used from our previous Evaluation definition,\n",
    "# but let's restate the source and testing criteria again.\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA, \n",
    "    \"include_sample_schema\": True, # Note this change! Needed for data gen.\n",
    "    \"type\": \"custom\"\n",
    "}\n",
    "# OPTIMIZATION 3: Multi-grader validation to reduce bias\n",
    "TESTING_CRITERIA = [\n",
    "    {\n",
    "        \"name\": \"Zava Tone Grader (Primary)\",\n",
    "        \"type\": \"score_model\", \n",
    "        \"model\": GRADER_MODEL,\n",
    "        \"input\": INPUT,\n",
    "        \"range\": [1.0, 10.0],\n",
    "        \"pass_threshold\": 4.0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Consistency Check (Secondary)\",\n",
    "        \"type\": \"score_model\",\n",
    "        \"model\": \"gpt-4o\",  # Different model for cross-validation\n",
    "        \"input\": INPUT,\n",
    "        \"range\": [1.0, 10.0], \n",
    "        \"pass_threshold\": 4.0,\n",
    "    }\n",
    "]\n",
    "posttraining_eval = client.evals.create(\n",
    "    name=f\"zava-tone-posttrain-evaluation-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=TESTING_CRITERIA  # Now using list of criteria\n",
    ")\n",
    "print(f\"Created eval {posttraining_eval.id}\")\n",
    "\n",
    "# Now add our runs.\n",
    "postraining_runs = []\n",
    "for model in POST_EVAL_MODELS:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": { \"type\": \"file_id\", \"id\": posttraining_file.id },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": SYSTEM_PROMPT },\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": { \"max_completions_tokens\": 100 }, # XXX again, note the purposeful typo\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"{model}-{UNIQUE_ENOUGH_KEY}\", \n",
    "        eval_id=posttraining_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE, \n",
    "    )\n",
    "    print(f\"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for {posttraining_eval.id}\")\n",
    "    postraining_runs.append(run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a5347",
   "metadata": {},
   "source": [
    "### 3.5 Wait For Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf13c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we wait for our runs to finish.\n",
    "start_time = time.time()\n",
    "\n",
    "while any([r.status not in [\"completed\", \"failed\"] for r in postraining_runs]):\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    for i in range(len(postraining_runs)):\n",
    "        postraining_runs[i] = client.evals.runs.retrieve(eval_id=posttraining_eval.id, run_id=postraining_runs[i].id)\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Run {postraining_runs[i].name}: {postraining_runs[i].status}\")\n",
    "    \n",
    "    now = time.time()\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((now - start_time) // 60), int((now - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ All {len(postraining_runs)} runs completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20554b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Interpret The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"eval_utils\", \"eval_utils.py\")\n",
    "eval_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(eval_utils)\n",
    "display_evaluation_summary = eval_utils.display_evaluation_summary\n",
    "\n",
    "display_evaluation_summary(client, [posttraining_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ebaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now let's put it all together.\n",
    "# Let's visualize our post-training evaluation. Fingers crossed!\n",
    "display_evaluation_summary(client, [baseline_eval.id, posttraining_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5aec1",
   "metadata": {},
   "source": [
    "## Step 6: Summary & Distillation Results\n",
    "\n",
    "Congratulations! You've successfully implemented **end-to-end model distillation** using Azure AI Foundry's integrated evaluation and fine-tuning pipeline.\n",
    "\n",
    "### üèÜ Key Achievements\n",
    "\n",
    "**Knowledge Transfer Success**: Successfully distilled specialized Zava customer service capabilities from a large reasoning model (GPT-4) to a much smaller, efficient model (GPT-4.1-nano).\n",
    "\n",
    "### üéØ Technical Accomplishments\n",
    "\n",
    "1. **‚úÖ Teacher Model Selection**: Identified optimal source model through custom grader evaluation\n",
    "2. **‚úÖ Training Data Generation**: Created high-quality distillation dataset from teacher outputs  \n",
    "3. **‚úÖ Student Model Training**: Fine-tuned smaller model using distillation techniques\n",
    "4. **‚úÖ Quality Validation**: Verified knowledge transfer effectiveness through consistent evaluation\n",
    "5. **‚úÖ Performance Optimization**: Achieved significant cost and latency improvements\n",
    "\n",
    "### üìä Business Impact\n",
    "\n",
    "- **ü§ë Cost Reduction**: Minimize per-token costs through smaller model deployment\n",
    "- **üèéÔ∏è Performance Boost**: Improved response latency for better customer experience  \n",
    "- **üìà Scalability**: Enable higher-volume customer service with efficient resource usage\n",
    "- **üéØ Quality Maintained**: Preserved Zava's tone and style standards through systematic evaluation\n",
    "\n",
    "### üß† Innovation Highlights\n",
    "\n",
    "**Automated Knowledge Transfer**: Achieved distillation without manual training data creation - only by defining evaluation criteria and letting AI systems handle the knowledge transfer process.\n",
    "\n",
    "**Integrated Pipeline**: Combined Azure AI Evaluations and Fine-Tuning in a seamless workflow that automatically:\n",
    "1. Identifies ideal responses from a capable reasoning model\n",
    "2. Uses custom graders to evaluate quality consistently  \n",
    "3. Transfers knowledge through fine-tuning to efficient models\n",
    "\n",
    "**Measurable Success**: Used objective grading throughout to validate that distillation preserved quality while optimizing for cost and performance.\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ You've mastered advanced model optimization through intelligent distillation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197dc0d6",
   "metadata": {},
   "source": [
    "## Step 7: Next Steps & Advanced Applications\n",
    "\n",
    "You've successfully completed model distillation! Here are your next steps for production deployment and advanced optimization:\n",
    "\n",
    "### üöÄ Production Deployment\n",
    "\n",
    "- **Scale Up Deployment**: Move from Developer to Standard tier for production workloads\n",
    "- **Load Testing**: Validate performance under realistic customer service volumes\n",
    "- **Cost Monitoring**: Track actual savings vs. original model deployment\n",
    "- **A/B Testing**: Compare distilled model against original in real customer scenarios\n",
    "\n",
    "### üìà Advanced Distillation Techniques\n",
    "\n",
    "- **Multi-Teacher Distillation**: Combine knowledge from multiple teacher models\n",
    "- **Progressive Distillation**: Use intermediate-size models for step-wise optimization\n",
    "- **Domain-Specific Distillation**: Apply techniques to other business areas (sales, support, technical)\n",
    "- **Cross-Modal Distillation**: Extend to image-text models for product recommendation\n",
    "\n",
    "### üîß Optimization & Monitoring\n",
    "\n",
    "- **Continuous Evaluation**: Set up automated quality monitoring with custom graders\n",
    "- **Model Drift Detection**: Track performance degradation over time\n",
    "- **Feedback Loops**: Incorporate customer satisfaction data into evaluation criteria\n",
    "- **Dynamic Re-distillation**: Automatically refresh models when quality drops\n",
    "\n",
    "### üåê Enterprise Integration\n",
    "\n",
    "- **MLOps Pipeline**: Integrate with Azure ML and DevOps for automated deployment\n",
    "- **Agent Framework**: Combine with LangChain, Semantic Kernel, or custom frameworks\n",
    "- **Edge Deployment**: Further optimize for edge computing environments\n",
    "- **Multi-Language Support**: Extend distillation to multilingual customer service\n",
    "\n",
    "### üìö Further Learning\n",
    "\n",
    "- **Model Compression**: Explore quantization and pruning for additional efficiency\n",
    "- **Reinforcement Learning**: Use RLHF for further quality improvements\n",
    "- **Custom Architectures**: Investigate task-specific model architectures\n",
    "- **Evaluation Science**: Develop more sophisticated custom graders and metrics\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Distillation Success Validation\n",
    "\n",
    "Your implementation should demonstrate:\n",
    "- **Quality Preservation**: Student model scores comparable to teacher on custom grader\n",
    "- **Performance Improvement**: Measurable gains in response time and cost efficiency  \n",
    "- **Scalability**: Ability to handle increased customer service volume\n",
    "- **Consistency**: Reliable adherence to Zava's brand standards\n",
    "\n",
    "**Ready to revolutionize your AI deployment strategy with intelligent model optimization? The foundation is now in place!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
