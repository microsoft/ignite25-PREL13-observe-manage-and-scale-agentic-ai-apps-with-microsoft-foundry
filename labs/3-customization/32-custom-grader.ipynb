{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44644603",
   "metadata": {},
   "source": [
    "# üéØ | Cora-For-Zava: Cost-Effective Model Distillation\n",
    "\n",
    "Welcome! This notebook will guide you through creating a custom grader and using model distillation to transfer knowledge from a large, expensive model to a smaller, faster, and more cost-effective one.\n",
    "\n",
    "## üõí Our Zava Scenario\n",
    "\n",
    "**Cora** is a customer service chatbot for **Zava** - a fictitious retailer of home improvement goods for DIY enthusiasts. While our fine-tuned model provides excellent tone and style, it's using a large, expensive model (GPT-4) for a focused task. Model distillation allows us to transfer this specialized knowledge to a smaller model (GPT-4.1-nano) for better cost-efficiency and performance.\n",
    "\n",
    "## üéØ What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll have:\n",
    "- ‚úÖ Created a custom grader to evaluate response quality consistently\n",
    "- ‚úÖ Established baseline \"gold standard\" responses for evaluation\n",
    "- ‚úÖ Defined clear grading criteria for tone and style assessment\n",
    "- ‚úÖ Used distillation to transfer knowledge to a smaller model\n",
    "- ‚úÖ Measured performance improvements in cost and latency\n",
    "- ‚úÖ Validated that the distilled model maintains quality standards\n",
    "\n",
    "## üí° What You'll Learn\n",
    "\n",
    "- How to design custom evaluators for specific business criteria\n",
    "- How to create effective grading rubrics for AI model assessment\n",
    "- How model distillation transfers knowledge from teacher to student models\n",
    "- How to balance model performance with cost and speed requirements\n",
    "- How to measure and validate distillation effectiveness\n",
    "- When to use distillation vs. fine-tuning vs. prompt engineering\n",
    "\n",
    "> **Key Insight**: Model distillation enables you to achieve specialized performance with smaller, faster, cheaper models by transferring knowledge from larger, more capable \"teacher\" models.\n",
    "\n",
    "Ready to optimize your model for cost and performance? Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd8483",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment Variables\n",
    "\n",
    "The following environment variables should already be configured in your `.env` file from the earlier setup steps:\n",
    "\n",
    "- **AZURE_OPENAI_API_KEY**: Your Azure OpenAI API key\n",
    "- **AZURE_OPENAI_ENDPOINT**: Your Azure OpenAI service endpoint\n",
    "- **AZURE_OPENAI_API_VERSION**: The API version to use\n",
    "- **AZURE_SUBSCRIPTION_ID**: Your Azure subscription ID\n",
    "- **AZURE_RESOURCE_GROUP**: Your Azure resource group name\n",
    "- **AZURE_AI_PROJECT_NAME**: Your Azure AI Foundry project name\n",
    "\n",
    "> **Note**: Model distillation requires access to both large teacher models (GPT-4) and smaller student models (GPT-4.1-nano) for optimal cost-performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56955b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure required environment variables are set\n",
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "model_name = \"gpt-4.1\"\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-02-01-preview\")\n",
    "\n",
    "if not openai_key or not openai_endpoint:\n",
    "    print(\"Error: Missing AZURE_OPENAI_KEY or AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "print(\"Using Model:\", model_name)\n",
    "print(\"Using API Version:\", api_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465864fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create an Azure OpenAI Client instance\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-04-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ea17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And create a handy UUID we can use to track each run of this notebook\n",
    "import uuid\n",
    "UNIQUE_ENOUGH_KEY = str(uuid.uuid4()).split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce633f",
   "metadata": {},
   "source": [
    "## Step 2: Building Our Custom Grader\n",
    "\n",
    "Model distillation involves transferring knowledge from a \"teacher\" model to a \"student\" model, enabling the student to perform specialized tasks with comparable accuracy but better efficiency. To measure distillation effectiveness, we need a consistent evaluation method.\n",
    "\n",
    "### üéØ The Distillation Process\n",
    "\n",
    "**Teacher Model** (GPT-4) ‚Üí **Knowledge Transfer** ‚Üí **Student Model** (GPT-4.1-nano)\n",
    "- Large, capable, expensive ‚Üí Specialized knowledge ‚Üí Small, fast, cost-effective\n",
    "\n",
    "### üèóÔ∏è Building an Effective Grader\n",
    "\n",
    "We'll create a custom grader through three key steps:\n",
    "\n",
    "1. **Curate Baseline Data**: Establish \"gold standard\" questions and responses that exemplify ideal Zava customer service tone and style\n",
    "2. **Define Grading Criteria**: Create clear, consistent evaluation rubrics that the grader will use to score responses\n",
    "3. **Validate Assessment**: Test our grader on the gold standard to ensure reliable and accurate evaluation\n",
    "\n",
    "### ‚úÖ Success Criteria\n",
    "\n",
    "A good grader will:\n",
    "- **Consistently** rank high-quality responses highly\n",
    "- **Reliably** identify tone and style issues\n",
    "- **Objectively** measure improvement after distillation\n",
    "- **Enable** fair comparison between teacher and student models\n",
    "\n",
    "By the end of this step, we'll have a robust grader ready to evaluate our distillation results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1f178",
   "metadata": {},
   "source": [
    "### Step 2.1: Curate Baseline Responses\n",
    "\n",
    "Create a \"gold standard\" dataset from our fine-tuning training data. These responses represent the ideal Zava customer service tone and style that we want our distilled model to achieve.\n",
    "\n",
    "**Data Source**: We'll use a subset of our successful fine-tuning training data (`distill_sft_baseline.json`) since these examples already demonstrate the desired response patterns.\n",
    "\n",
    "**Quality Criteria**: Each baseline response should exhibit:\n",
    "- ‚úÖ Emoji-led opening for friendliness\n",
    "- ‚úÖ Polite acknowledgment of customer needs\n",
    "- ‚úÖ Factual product information and pricing\n",
    "- ‚úÖ Helpful follow-up questions or offers\n",
    "\n",
    "This baseline dataset will be uploaded to Azure AI Foundry for use in our grader evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c465496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSONL file into a DataFrame and print the first 5 rows in 2 columns\n",
    "baseline_jsonl_df = pd.read_json(\"33-distill_sft_baseline.jsonl\", lines=True)\n",
    "# Display all columns and set display width to show full text in the output\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\"question\": row[\"item\"][\"question\"], \"answer\": row[\"item\"][\"answer\"]}\n",
    "        for _, row in baseline_jsonl_df.head(5).iterrows()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2e925",
   "metadata": {},
   "source": [
    "### Step 2.2: Upload Baseline Data To Azure\n",
    "\n",
    "Upload our curated baseline dataset to Azure AI Foundry for use in the custom grader evaluation. This data will serve as our \"gold standard\" for measuring grader effectiveness.\n",
    "\n",
    "**Upload Process**:\n",
    "- **Secure Storage**: Data is stored in your Azure AI Foundry project\n",
    "- **Version Control**: Maintains data provenance for reproducible evaluations  \n",
    "- **Access Control**: Only accessible within your project scope\n",
    "- **Integration Ready**: Formatted for seamless use with Azure AI evaluation services\n",
    "\n",
    "> **Best Practice**: Always validate data quality before uploading to ensure consistent grader performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20078f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cloud-hosted file with baseline data for evaluations (purpose=\"evals\")\n",
    "# An \"evals\" file is a special type of file that can be used in evaluation jobs \n",
    "# - it has to be in JSONL format but its properties depend on how grader is setup\n",
    "grader_eval_file = None\n",
    "with open(\"./33-distill_baseline.jsonl\", \"rb\") as f:\n",
    "    grader_eval_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    grader_eval_file = client.files.wait_for_processing(grader_eval_file.id)\n",
    "\n",
    "print(f\"Created eval file:\\n{grader_eval_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132b0c4",
   "metadata": {},
   "source": [
    "### Step 2.3: Define Custom Grading Criteria\n",
    "\n",
    "Create a specialized evaluator that assesses Zava customer service quality using our specific business criteria. This is an example of **AI-Assisted Evaluation** (LLM-as-a-Judge) where we use a capable reasoning model to score responses based on our custom rubric.\n",
    "\n",
    "**Grading Approach**:\n",
    "- **Input**: Question-answer pairs from customer interactions\n",
    "- **Output**: Structured scores for polite, helpful, and informative metrics\n",
    "- **Method**: Multi-criteria evaluation with weighted scoring\n",
    "- **Consistency**: Same grader used throughout distillation pipeline\n",
    "\n",
    "**Scoring Rubric**:\n",
    "1. **Politeness (1-5)**: Emoji usage, greetings, acknowledgment of customer needs\n",
    "2. **Helpfulness (1-5)**: Relevant follow-up offers and assistance\n",
    "3. **Information (0-1)**: Specific product mentions and factual details\n",
    "4. **Final Score**: `(Politeness + Helpfulness) √ó Information`\n",
    "\n",
    "> **Key Design**: Information score acts as a gate - responses without product details receive zero final score, regardless of tone quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39987c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the most capable reasoning model as our grader.\n",
    "GRADER_MODEL = \"o4-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create a custom Grader that will score the responses for \"Helpfulness\" on a scale of 0-10\n",
    "# using the custom rubric below - where 0 is \"not helpful at all\" and 10 is \"extremely helpful\".\n",
    "\n",
    "# We want it to grader our \"student\" responses later, based on the same criteria. So let's make sure we get our rubric right.\n",
    "# We want good answers to score high - and bad answers to be penalized heavily (get 0)\n",
    "\n",
    "GRADER_PROMPT = \"\"\"\n",
    "\n",
    "You are an expert in assessing polite and helpful customer service responses\n",
    "\n",
    "You'll be given a conversation in the form of a question and answer. \n",
    "\n",
    "## Scoring Criteria\n",
    "\n",
    "Judge the answer by using two metrics to compute a final score.\n",
    "\n",
    "### Metric 1: Is the answer polite?\n",
    "\n",
    "Give this a score in the range of 1 to 5 where:\n",
    "- 1 means the answer was rude, disrespectful or dismissive\n",
    "- 3 means the answer was neutral, neither polite nor rude\n",
    "- 5 means the answer had an emoji followed by a greeting or an acknowledgement of the user question\n",
    "\n",
    "### Metric 2: Is the answer helpful?\n",
    "\n",
    "Give this a score in the range of 1 to 5 where:\n",
    "- 1 means the response did not end with an offer to help further\n",
    "- 3 means the response ended with a generic offer to help\n",
    "- 5 means the response ended with an offer to help that was clearly related to the user's question\n",
    "\n",
    "### Metric 3: Is the answer informative?\n",
    "Give this a score of 0 or 1 where:\n",
    "- 0 means the answer did not mention any specific product or product-related fact\n",
    "- 1 means the answer mentioned a specific product or solution\n",
    "\n",
    "### Final Score\n",
    "The final score you must decide should be based on a weighted blend of Metric 1 and\n",
    "Metric 2 using the formula: `(Metric 1 + Metric 2) * (Metric 3)`\n",
    "\n",
    "This means that if Metric 3 is zero, the final score must be zero.\n",
    "\n",
    "## Response Structure\n",
    "Your response must be in a JSON format that can be loaded by Python's json.loads()\n",
    "function. It must resemble the following:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"steps\": [\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 1>\", \n",
    "      \"result\": <string representation of Metric 1's score> \n",
    "    },\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 2>\", \n",
    "      \"result\": <string representation of Metric 2's score> \n",
    "    }\n",
    "  ],\n",
    "  \"result\": <floating point value of the Final Score>\n",
    "}\n",
    "\n",
    "## General Guidance\n",
    "The questions will be about paint products and related topics. Deep research is not required. Use common sense to determine if the answer is polite, helpful and factual. The responses should be concise and to the point.\n",
    "\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac3299",
   "metadata": {},
   "source": [
    "### Step 2.4: Implement the Custom Grader\n",
    "\n",
    "Now we'll combine our grading criteria with the Azure AI Evaluation SDK to create a functional custom evaluator. This grader will:\n",
    "\n",
    "**Core Functions**:\n",
    "- **Parse Responses**: Extract questions and answers from conversation data\n",
    "- **Apply Criteria**: Use our polite/helpful/factual scoring rubric\n",
    "- **Generate Scores**: Provide consistent 1-5 ratings for each metric\n",
    "- **Calculate Final Score**: Combine individual metrics into an overall quality assessment\n",
    "\n",
    "**Integration**: The custom grader integrates with Azure AI Foundry's evaluation pipeline, allowing us to measure model performance consistently before and after distillation.\n",
    "\n",
    "> **Quality Assurance**: This grader will be our standard measurement tool throughout the distillation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our Evaluation for validating our choice in grader prompt and model.\n",
    "\n",
    "# The entire user prompt is data driven from the file. No generation is done using\n",
    "# a model in this case, just simple string substitution using this pattern. This\n",
    "# means we directly reference the two fields in our baseline.jsonl file.\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{item.answer}}\n",
    "\"\"\"\n",
    "\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# We need to describe what our evaluation dataset looks like.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    }\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA,\n",
    "    \"include_sample_schema\": False,\n",
    "    \"type\": \"custom\",\n",
    "}\n",
    "\n",
    "# Lastly, we define test criteria that combines all the above.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Zava Tone Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],    # Our grader scores in a range from 1 to 10\n",
    "    \"pass_threshold\": 4.0,   # Let's say a 4 is \"passing\" for now.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3f23b",
   "metadata": {},
   "source": [
    "### 2.5 Submit The Evaluation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've set up the parameters for our Eval, now we create it via the API.\n",
    "grader_eval = client.evals.create(\n",
    "    name=f\"zava-tone-baseline-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA],\n",
    ")\n",
    "\n",
    "print(f\"‚öñÔ∏è Submitted grader evaluation {grader_eval.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadff41",
   "metadata": {},
   "source": [
    "### 2.6 Run The Evaluation Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our evaluation needs a test run. This is where we let it know to use our\n",
    "# \"gold standard\" file (baseline.jsonl) to test our grader.\n",
    "\n",
    "RUN_DATA_SOURCE = {\n",
    "    \"type\": \"jsonl\",\n",
    "    \"source\": { \"type\": \"file_id\", \"id\": grader_eval_file.id }\n",
    "}\n",
    "grader_run = client.evals.runs.create(\n",
    "    name=f\"32-zava-tone-grader-{GRADER_MODEL}\",\n",
    "    eval_id=grader_eval.id,\n",
    "    data_source=RUN_DATA_SOURCE,\n",
    ")\n",
    "print(f\"üèÉ‚Äç‚û°Ô∏è Submitted run {grader_run.id} for {grader_eval.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887047d3",
   "metadata": {},
   "source": [
    "### 2.7 Poll For Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Eval Run takes time to complete. Let's actively wait for it to finish before continuing.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)\n",
    "while grader_run.status not in [\"completed\", \"failed\"]:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    grader_run = client.evals.runs.retrieve(eval_id=grader_eval.id, run_id=grader_run.id)\n",
    "    now = time.time()\n",
    "    mins, secs = int((now - start_time) // 60), int((now - start_time) % 60)\n",
    "    print(f\"‚è±Ô∏è Elapsed time: {mins} minutes {secs} seconds\")\n",
    "\n",
    "print(f\"üèÅ Run {grader_run.id}: {grader_run.status}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3407dd",
   "metadata": {},
   "source": [
    "### 2.8 View & Analyze Results\n",
    "\n",
    "You can see these on the Azure AI Foundry portal - or run a script to visualize them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've got a handy script for rendering the results from an Evaluations Runs. Let's\n",
    "# eyeball this stuff. It uses the Evals API to retrieve the scores and plot them.\n",
    "from eval_utils import display_evaluation_summary\n",
    "\n",
    "display_evaluation_summary(client, [grader_eval.id], x_range=(0, 10))\n",
    "\n",
    "# We should see that our grader generally thinks our \"gold standard\" is pretty on-brand for Zava tone. \n",
    "# This is where we'd iterate on tuning the grader, making sure we\n",
    "# clearly capture features for it to score, etc. We're keeping it simple for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b76fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Baseline Testing Our Candidate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fb56d",
   "metadata": {},
   "source": [
    "### 3.1 Curate Q&A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea010600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSONL file into a DataFrame and print the first 5 rows in 2 columns\n",
    "qa_jsonl_df = pd.read_json(\"33-distill_sft_qa.jsonl\", lines=True)\n",
    "# Display all columns and set display width to show full text in the output\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.DataFrame(\n",
    "    [\n",
    "        {\"question\": row[\"item\"][\"question\"], \"answer\": row[\"item\"][\"answer\"]}\n",
    "        for _, row in qa_jsonl_df.head(5).iterrows()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc388ad7",
   "metadata": {},
   "source": [
    "### 3.2 Split Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12911fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "qa = []\n",
    "with open(\"33-distill_sft_qa.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        qa.append(json.loads(line))\n",
    "\n",
    "print(f\"Number of Q/A pairs: {len(qa)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8726f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we'll randomize it to maybe prove a point that this isn't totally staged üòú\n",
    "from random import shuffle\n",
    "shuffle(qa)\n",
    "\n",
    "# Now we split 50/50.\n",
    "split_at = int(len(qa) / 2)\n",
    "qa_baseline = qa[:split_at]\n",
    "qa_validation = qa[split_at:]\n",
    "\n",
    "# Check it.\n",
    "print(f\"{len(qa_baseline)} pairs for baseline testing, {len(qa_validation)} for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010febd",
   "metadata": {},
   "source": [
    "### 3.3 Upload baseline (training) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll upload our baseline dataset and prepare our Evaluation. We need to save the data\n",
    "# to disk first for...reasons...because of the OpenAI SDK. That's fine.\n",
    "filename = f\"./32-zava-tone-baseline-{UNIQUE_ENOUGH_KEY}.jsonl\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for row in qa_baseline:\n",
    "        json.dump(row, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "baseline_file = None\n",
    "with open(filename, \"rb\") as f:\n",
    "    baseline_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    baseline_file = client.files.wait_for_processing(baseline_file.id)\n",
    "\n",
    "print(f\"Created baseline file:\\n{baseline_file.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e269ad0",
   "metadata": {},
   "source": [
    "### 3.4 Create The Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf281e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now build out the Evaluation details. In this case, we'll *generate* responses\n",
    "# using a base model, unlike before where we used the pre-canned results just to test\n",
    "# the grader.\n",
    "\n",
    "# We'll use a simple system prompt to show how distillation and fine-tuning let us\n",
    "# get away without overly complex prompt engineering.\n",
    "SYSTEM_PROMPT = \"You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store.\"\n",
    "\n",
    "# We'll use a flee of base models as our baseline, including `o3-mini` (our grader).\n",
    "BASE_MODELS = [\n",
    "    \"o3-mini\",\n",
    "    \"o4-mini\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4.1-nano\",\n",
    "]\n",
    "\n",
    "# The prompt we'll grade will look like this pattern. Similar to before, but now we're\n",
    "# going to use {{sample.output_text}} to substitute what the model under test generates.\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{sample.output_text}}\n",
    "\"\"\"\n",
    "\n",
    "# Input to our grader remains the same as before, but we reproduce it here for context.\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"system\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# The schema and data source are similar, but with one major difference noted below.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    },\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA, \n",
    "    \"include_sample_schema\": True, # Note this change! Needed for data gen.\n",
    "    \"type\": \"custom\"\n",
    "}\n",
    "\n",
    "# Same testing criteria, reproduced again for context.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Zava Tone Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],\n",
    "    \"pass_threshold\": 4.0,\n",
    "}\n",
    "\n",
    "# We create one Evaluation for *all* our base models. Each model is tested in a\n",
    "# distinct Run that we'll define next.\n",
    "baseline_eval = client.evals.create(\n",
    "    name=f\"32-zava-tone-baseline-{UNIQUE_ENOUGH_KEY}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA]\n",
    ")\n",
    "print(f\"‚öñÔ∏è Created baseline eval {baseline_eval.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ac8b7",
   "metadata": {},
   "source": [
    "### 3.5 Run Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each run gets its own data source definition as it needs to specify a different\n",
    "# model deployment to use for generation. The template is the prompt template\n",
    "# sent to the model under test. It uses the simple Clippy system prompt and for\n",
    "# the user input, we use the \"question\" from the baseline Q&A data file.\n",
    "baseline_runs = []\n",
    "for model in BASE_MODELS:\n",
    "    RUN_DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": { \"type\": \"file_id\", \"id\": baseline_file.id },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": SYSTEM_PROMPT },\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": { \"max_completions_tokens\": 20_000 } if model.startswith(\"o\") else { \"max_completions_tokens\": 100 }, # XXX\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"{model}-{UNIQUE_ENOUGH_KEY}\", \n",
    "        eval_id=baseline_eval.id,\n",
    "        data_source=RUN_DATA_SOURCE, \n",
    "    )\n",
    "    print(f\"üèÉ‚Äç‚û°Ô∏è Created run {run.id} for eval {baseline_eval.id}\")\n",
    "    baseline_runs.append(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4f158",
   "metadata": {},
   "source": [
    "### 3.6 Poll For Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have to wait for our half-dozen or so Runs to finish. Twiddle your thumbs a bit!\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while any([r.status not in [\"completed\", \"failed\"] for r in baseline_runs]):\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    for i in range(len(baseline_runs)):\n",
    "        baseline_runs[i] = client.evals.runs.retrieve(eval_id=baseline_eval.id, run_id=baseline_runs[i].id)\n",
    "        print(f\"üèÉ‚Äç‚û°Ô∏è Run {baseline_runs[i].name}: {baseline_runs[i].status}\")\n",
    "    \n",
    "    now = time.time()\n",
    "    print(\"‚è±Ô∏è Elapsed time: {} minutes {} seconds\".format(int((now - start_time) // 60), int((now - start_time) % 60)))\n",
    "\n",
    "print(f\"üèÅ All {len(baseline_runs)} runs completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39495ddb",
   "metadata": {},
   "source": [
    "### 3.7 Visualize Results & Pick Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77608b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our evaluation and identify the best and worst performers.\n",
    "display_evaluation_summary(client, [baseline_eval.id], x_range=(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Save the evaluation objects and runs\n",
    "export_data = {\n",
    "    'baseline_eval_id': baseline_eval.id,\n",
    "    'baseline_runs_ids': [run.id for run in baseline_runs],\n",
    "    'baseline_runs_models': [run.model for run in baseline_runs],\n",
    "    'qa_validation': qa_validation,\n",
    "    'GRADER_MODEL': GRADER_MODEL,\n",
    "    'GRADER_PROMPT': GRADER_PROMPT,\n",
    "    'SYSTEM_PROMPT': SYSTEM_PROMPT,\n",
    "    'UNIQUE_ENOUGH_KEY': UNIQUE_ENOUGH_KEY\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "with open('32-distillation_export.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Exported data for distillation notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5aec1",
   "metadata": {},
   "source": [
    "## Step 8: Summary & Results\n",
    "\n",
    "Congratulations! You've successfully implemented **model distillation** using Azure AI Foundry's evaluation and fine-tuning capabilities. Here's what you accomplished:\n",
    "\n",
    "### üéØ Distillation Achievement\n",
    "\n",
    "**Teacher ‚Üí Student Transfer**: Successfully distilled specialized Zava customer service knowledge from a large reasoning model (GPT-4) into a much smaller, faster model (GPT-4.1-nano) while maintaining quality standards.\n",
    "\n",
    "### üèÜ Key Accomplishments\n",
    "\n",
    "1. **‚úÖ Custom Grader Creation**: Built a reliable evaluator using AI-assisted evaluation (LLM-as-a-Judge) with business-specific criteria\n",
    "2. **‚úÖ Baseline Establishment**: Curated gold standard responses from successful fine-tuning data\n",
    "3. **‚úÖ Quality Measurement**: Validated grader effectiveness on baseline data\n",
    "4. **‚úÖ Knowledge Transfer**: Used distillation to transfer specialized capabilities to smaller model\n",
    "5. **‚úÖ Performance Optimization**: Achieved cost and latency improvements while preserving quality\n",
    "\n",
    "### üìä Business Impact\n",
    "\n",
    "- **ü§ë Cost Reduction**: Smaller models use fewer tokens per inference, significantly reducing operational costs\n",
    "- **üèéÔ∏è Performance Boost**: Faster response times improve customer experience and system throughput  \n",
    "- **üéØ Quality Maintained**: Custom grader ensures distilled model meets Zava's tone and style standards\n",
    "- **üìà Scalability**: Efficient model allows handling higher customer volumes\n",
    "\n",
    "### üß† Technical Innovation\n",
    "\n",
    "**No Manual Training Data**: Achieved distillation without creating training examples manually - just by defining evaluation criteria and letting AI systems handle the knowledge transfer process.\n",
    "\n",
    "**Automated Pipeline**: Combined Azure AI Evaluations and Fine-Tuning to create an automated distillation workflow.\n",
    "\n",
    "**Objective Measurement**: Used consistent grading criteria throughout the process to validate improvement.\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ You've mastered the art of cost-effective AI model optimization through distillation!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
