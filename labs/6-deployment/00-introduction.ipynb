{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e94201",
   "metadata": {},
   "source": [
    "# ðŸš€ Introduction to AI Agent Deployment\n",
    "\n",
    "## ðŸ›’ The Zava Scenario\n",
    "\n",
    "Cora has been evaluated, tested, and is ready for customers. But deploying an AI agent to production is different from deploying traditional applications. How do we ensure Cora can handle production traffic? What about managing costs and updates?\n",
    "\n",
    "**The Reality**: Production deployment requires careful planning around capacity management, monitoring, rollout strategies, and operational readiness.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this section, you'll understand:\n",
    "\n",
    "1. **Model deployment fundamentals** - From development to production\n",
    "2. **Capacity planning** - PTUs vs. pay-per-use pricing models\n",
    "3. **Deployment strategies** - Blue-green and canary deployments\n",
    "4. **Monitoring and observability** - Tracking performance in production\n",
    "5. **Production readiness checklist** - Key considerations before launch\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Successful deployment ensures:\n",
    "- **Reliability** - Cora is available when customers need it\n",
    "- **Performance** - Fast response times even under load\n",
    "- **Cost optimization** - Right-sized infrastructure for your needs\n",
    "- **Safe updates** - Rolling out improvements without disruption\n",
    "- **Operational excellence** - Monitoring and incident response capabilities\n",
    "\n",
    "Let's explore how to deploy Cora to production with confidence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdabdd8",
   "metadata": {},
   "source": [
    "## What is Model Deployment?\n",
    "\n",
    "**Model deployment** is the process of making your AI model available for production use.\n",
    "\n",
    "### Development vs. Production\n",
    "\n",
    "**Development environment:**\n",
    "- Test models locally or in notebooks\n",
    "- Experiment with different configurations\n",
    "- Limited users (just you and your team)\n",
    "- Failures are acceptable (learning process)\n",
    "- Can restart/change anytime\n",
    "\n",
    "**Production environment:**\n",
    "- Model serves real customers\n",
    "- Must be reliable and performant\n",
    "- High availability required\n",
    "- Failures impact business\n",
    "- Changes must be carefully managed\n",
    "\n",
    "**Deployment bridges this gap.**\n",
    "\n",
    "### What Gets Deployed\n",
    "\n",
    "**For base models:**\n",
    "- Model already deployed by Azure\n",
    "- You just reference it (e.g., \"gpt-4o-mini\")\n",
    "- No deployment action needed\n",
    "\n",
    "**For fine-tuned models:**\n",
    "- Custom model created by your fine-tuning job\n",
    "- Exists in Azure but not yet accessible\n",
    "- Must be **deployed** to make it available via API\n",
    "- Gets a deployment name you can reference\n",
    "\n",
    "**Analogy:**\n",
    "```\n",
    "Fine-tuning = Building a custom car\n",
    "Deployment = Putting it on the road for use\n",
    "```\n",
    "\n",
    "### The Deployment Process\n",
    "\n",
    "```\n",
    "1. Fine-Tuning Job Completes\n",
    "   â†“\n",
    "   Model created (fine-tuned-model-xyz)\n",
    "   \n",
    "2. Create Deployment\n",
    "   â†“\n",
    "   Deploy model to inference endpoint\n",
    "   Assign capacity (tokens per minute)\n",
    "   Name deployment (e.g., \"cora-v1\")\n",
    "   \n",
    "3. Deployment Ready\n",
    "   â†“\n",
    "   Model accessible via API\n",
    "   Can process customer requests\n",
    "   \n",
    "4. Start Using\n",
    "   â†“\n",
    "   Update application to use new deployment\n",
    "   Route traffic to deployed model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9cd4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Deploy Fine-Tuned Models?\n",
    "\n",
    "### The Problem Without Deployment\n",
    "\n",
    "**After fine-tuning:**\n",
    "```python\n",
    "# Your fine-tuning job completes\n",
    "job_status = \"succeeded\"\n",
    "model_id = \"ft:gpt-4o-mini:zava:abc123\"\n",
    "\n",
    "# Try to use it\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,  # This won't work yet!\n",
    "    messages=[...]\n",
    ")\n",
    "# â†’ Error: Model not found or not deployed\n",
    "```\n",
    "\n",
    "**The fine-tuned model exists but isn't deployed.**\n",
    "\n",
    "### After Deployment\n",
    "\n",
    "```python\n",
    "# Deploy the model first\n",
    "deployment_name = \"cora-fine-tuned-v1\"\n",
    "\n",
    "# Now you can use it\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,  # âœ“ Works!\n",
    "    messages=[...]\n",
    ")\n",
    "```\n",
    "\n",
    "**Deployment makes the model accessible.**\n",
    "\n",
    "### Benefits of Proper Deployment\n",
    "\n",
    "**1. API Access**\n",
    "- Model available via standard API\n",
    "- Consistent interface with other models\n",
    "- Easy to integrate into applications\n",
    "\n",
    "**2. Capacity Management**\n",
    "- Control tokens per minute (TPM)\n",
    "- Scale up/down as needed\n",
    "- Prevent overload\n",
    "\n",
    "**3. Version Control**\n",
    "- Multiple deployments for same model\n",
    "- A/B testing (v1 vs. v2)\n",
    "- Rollback capability\n",
    "\n",
    "**4. Security & Access**\n",
    "- Managed identity integration\n",
    "- Role-based access control\n",
    "- Network isolation options\n",
    "\n",
    "**5. Monitoring**\n",
    "- Track usage and performance\n",
    "- Monitor costs\n",
    "- Detect issues early"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57744e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment Concepts\n",
    "\n",
    "### Deployment Name\n",
    "\n",
    "**What it is:** A friendly name for your deployed model\n",
    "\n",
    "**Purpose:**\n",
    "- Reference model in API calls\n",
    "- Distinguish between versions\n",
    "- Organize multiple deployments\n",
    "\n",
    "**Examples:**\n",
    "```python\n",
    "# Good deployment names\n",
    "\"cora-fine-tuned-v1\"\n",
    "\"zava-customer-service-prod\"\n",
    "\"product-assistant-2024-11\"\n",
    "\n",
    "# Less clear\n",
    "\"my-model\"\n",
    "\"test\"\n",
    "\"deployment1\"\n",
    "```\n",
    "\n",
    "**Best practice:** Use descriptive, versioned names\n",
    "\n",
    "### Model ID vs. Deployment Name\n",
    "\n",
    "**Model ID:**\n",
    "- Created by fine-tuning job\n",
    "- Immutable (doesn't change)\n",
    "- Example: `ft:gpt-4o-mini:zava:abc123::xyz789`\n",
    "\n",
    "**Deployment Name:**\n",
    "- Chosen by you when deploying\n",
    "- Mutable (can redeploy)\n",
    "- Example: `cora-v1`\n",
    "\n",
    "```python\n",
    "# Deploy model (one-time)\n",
    "create_deployment(\n",
    "    model=model_id,              # What to deploy\n",
    "    name=deployment_name         # What to call it\n",
    ")\n",
    "\n",
    "# Use model (many times)\n",
    "query_model(\n",
    "    model=deployment_name,       # Use friendly name\n",
    "    prompt=customer_question\n",
    ")\n",
    "```\n",
    "\n",
    "### Capacity and Scaling\n",
    "\n",
    "**Capacity = Tokens Per Minute (TPM)**\n",
    "\n",
    "**What it means:**\n",
    "- Maximum tokens your deployment can process per minute\n",
    "- Includes input + output tokens\n",
    "- Determines throughput\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Deployment capacity: 10,000 TPM\n",
    "\n",
    "Customer query: 100 tokens\n",
    "Typical response: 150 tokens\n",
    "Total per request: 250 tokens\n",
    "\n",
    "Max queries/minute: 10,000 / 250 = 40 queries\n",
    "Max queries/day: 40 Ã— 60 Ã— 24 = 57,600 queries\n",
    "```\n",
    "\n",
    "**Choosing capacity:**\n",
    "\n",
    "| Use Case | Recommended TPM | Reasoning |\n",
    "|----------|-----------------|-----------|\n",
    "| **Development/Testing** | 1,000 - 10,000 | Low volume, cost-effective |\n",
    "| **Small Production** | 10,000 - 50,000 | Hundreds of users |\n",
    "| **Medium Production** | 50,000 - 200,000 | Thousands of users |\n",
    "| **Large Production** | 200,000+ | High volume, enterprise scale |\n",
    "\n",
    "**Scaling strategies:**\n",
    "\n",
    "```python\n",
    "# Start small\n",
    "initial_capacity = 10_000  # TPM\n",
    "\n",
    "# Monitor usage\n",
    "actual_usage = get_usage_metrics()\n",
    "\n",
    "# Scale up if needed\n",
    "if actual_usage > 0.8 * initial_capacity:\n",
    "    new_capacity = initial_capacity * 2\n",
    "    update_deployment(capacity=new_capacity)\n",
    "```\n",
    "\n",
    "### Deployment Region\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "**1. Latency**\n",
    "- Deploy close to users\n",
    "- Reduces response time\n",
    "- Better user experience\n",
    "\n",
    "**2. Model Availability**\n",
    "- Not all models in all regions\n",
    "- Fine-tuning available in specific regions\n",
    "- Check availability before deploying\n",
    "\n",
    "**3. Compliance**\n",
    "- Data residency requirements\n",
    "- Regional regulations\n",
    "- Company policies\n",
    "\n",
    "**Example regions:**\n",
    "- East US\n",
    "- West Europe\n",
    "- Sweden Central (popular for fine-tuning)\n",
    "- North Central US\n",
    "\n",
    "**Best practice:** Deploy in same region as your fine-tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51e9b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment Process Step-by-Step\n",
    "\n",
    "### Step 1: Verify Fine-Tuning Job\n",
    "\n",
    "**Before deploying, ensure fine-tuning succeeded:**\n",
    "\n",
    "```python\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(...)\n",
    "\n",
    "# Get fine-tuning job\n",
    "job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "# Check status\n",
    "print(f\"Status: {job.status}\")\n",
    "print(f\"Model ID: {job.fine_tuned_model}\")\n",
    "\n",
    "# Verify success\n",
    "if job.status == \"succeeded\":\n",
    "    print(\"âœ“ Fine-tuning complete, ready to deploy\")\n",
    "    model_id = job.fine_tuned_model\n",
    "else:\n",
    "    print(f\"âœ— Job not ready: {job.status}\")\n",
    "```\n",
    "\n",
    "**Possible statuses:**\n",
    "- `succeeded` â†’ Ready to deploy âœ“\n",
    "- `running` â†’ Wait for completion\n",
    "- `failed` â†’ Check errors, retry\n",
    "- `cancelled` â†’ Start new job\n",
    "\n",
    "### Step 2: Create Deployment\n",
    "\n",
    "**Using Azure OpenAI SDK:**\n",
    "\n",
    "```python\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Connect to Azure\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=AZURE_SUBSCRIPTION_ID,\n",
    "    resource_group_name=AZURE_RESOURCE_GROUP,\n",
    "    workspace_name=AZURE_AI_PROJECT_NAME\n",
    ")\n",
    "\n",
    "# Create deployment\n",
    "deployment = ml_client.online_deployments.begin_create_or_update(\n",
    "    deployment_name=\"cora-fine-tuned-v1\",\n",
    "    model=model_id,\n",
    "    sku={\n",
    "        \"name\": \"Standard\",\n",
    "        \"capacity\": 10  # Tokens per minute (in thousands)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "deployment.wait()\n",
    "print(\"âœ“ Deployment ready\")\n",
    "```\n",
    "\n",
    "**Or using Azure Portal:**\n",
    "\n",
    "1. Go to Azure AI Foundry (https://ai.azure.com)\n",
    "2. Select your project\n",
    "3. Click \"Deployments\" â†’ \"Create deployment\"\n",
    "4. Choose your fine-tuned model\n",
    "5. Set deployment name and capacity\n",
    "6. Click \"Deploy\"\n",
    "\n",
    "### Step 3: Verify Deployment\n",
    "\n",
    "**Test the deployed model:**\n",
    "\n",
    "```python\n",
    "# Query deployed model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"cora-fine-tuned-v1\",  # Your deployment name\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are Cora, a Zava assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"What paint do you have?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "# â†’ Should respond in Cora's fine-tuned style\n",
    "```\n",
    "\n",
    "**Verification checklist:**\n",
    "- âœ“ Model responds without errors\n",
    "- âœ“ Response quality matches expectations\n",
    "- âœ“ Latency is acceptable (< 2-3 seconds)\n",
    "- âœ“ Style matches fine-tuned behavior\n",
    "\n",
    "### Step 4: Update Application\n",
    "\n",
    "**Point your application to the new deployment:**\n",
    "\n",
    "```python\n",
    "# Before (using base model)\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "# After (using fine-tuned deployment)\n",
    "MODEL_NAME = \"cora-fine-tuned-v1\"\n",
    "\n",
    "# Application code stays the same\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[...]\n",
    ")\n",
    "```\n",
    "\n",
    "**For gradual rollout:**\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def get_model_name():\n",
    "    \"\"\"Route 10% to new model, 90% to old\"\"\"\n",
    "    if random.random() < 0.1:\n",
    "        return \"cora-fine-tuned-v1\"  # New\n",
    "    else:\n",
    "        return \"gpt-4o-mini\"  # Old (base)\n",
    "\n",
    "# Use in application\n",
    "response = client.chat.completions.create(\n",
    "    model=get_model_name(),\n",
    "    messages=[...]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd2d85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Management Best Practices\n",
    "\n",
    "### 1. Version Your Deployments\n",
    "\n",
    "**Use semantic versioning:**\n",
    "\n",
    "```python\n",
    "# Version naming\n",
    "\"cora-v1.0\"  # Initial release\n",
    "\"cora-v1.1\"  # Minor update (bug fix)\n",
    "\"cora-v2.0\"  # Major update (new capabilities)\n",
    "\n",
    "# Date-based versioning\n",
    "\"cora-2024-11-13\"\n",
    "\"cora-november-prod\"\n",
    "\n",
    "# Environment-based\n",
    "\"cora-dev\"\n",
    "\"cora-staging\"\n",
    "\"cora-prod\"\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Clear version history\n",
    "- Easy rollback if needed\n",
    "- A/B testing between versions\n",
    "\n",
    "### 2. Maintain Multiple Deployments\n",
    "\n",
    "```python\n",
    "deployments = {\n",
    "    \"cora-prod\": {\n",
    "        \"model\": \"ft:gpt-4o-mini:zava:v1\",\n",
    "        \"capacity\": 50_000,\n",
    "        \"purpose\": \"Production traffic\"\n",
    "    },\n",
    "    \"cora-staging\": {\n",
    "        \"model\": \"ft:gpt-4o-mini:zava:v2\",\n",
    "        \"capacity\": 10_000,\n",
    "        \"purpose\": \"Testing new version\"\n",
    "    },\n",
    "    \"cora-dev\": {\n",
    "        \"model\": \"ft:gpt-4o-mini:zava:v3\",\n",
    "        \"capacity\": 1_000,\n",
    "        \"purpose\": \"Development experiments\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Workflow:**\n",
    "```\n",
    "1. Develop â†’ test in cora-dev\n",
    "2. Validate â†’ promote to cora-staging\n",
    "3. Production test â†’ small % to cora-staging\n",
    "4. Full rollout â†’ switch cora-prod\n",
    "```\n",
    "\n",
    "### 3. Implement Rollback Strategy\n",
    "\n",
    "**Quick rollback if issues occur:**\n",
    "\n",
    "```python\n",
    "# Configuration\n",
    "deployments = {\n",
    "    \"current\": \"cora-v2.0\",\n",
    "    \"previous\": \"cora-v1.0\"\n",
    "}\n",
    "\n",
    "# If v2.0 has issues\n",
    "def rollback():\n",
    "    \"\"\"Switch back to previous version\"\"\"\n",
    "    deployments[\"current\"] = deployments[\"previous\"]\n",
    "    print(f\"Rolled back to {deployments['current']}\")\n",
    "\n",
    "# Monitor for issues\n",
    "if error_rate > threshold:\n",
    "    rollback()\n",
    "```\n",
    "\n",
    "**Keep previous versions deployed for quick switching.**\n",
    "\n",
    "### 4. Monitor Deployment Health\n",
    "\n",
    "**Key metrics to track:**\n",
    "\n",
    "```python\n",
    "# Usage metrics\n",
    "total_requests = count_api_calls()\n",
    "tokens_used = sum_token_usage()\n",
    "error_rate = count_errors() / total_requests\n",
    "\n",
    "# Performance metrics\n",
    "avg_latency = mean_response_time()\n",
    "p95_latency = percentile_95_response_time()\n",
    "\n",
    "# Cost metrics\n",
    "daily_cost = tokens_used * price_per_token\n",
    "\n",
    "# Quality metrics\n",
    "satisfaction_score = average_user_rating()\n",
    "```\n",
    "\n",
    "**Set up alerts:**\n",
    "\n",
    "```python\n",
    "alerts = {\n",
    "    \"error_rate\": {\"threshold\": 0.05, \"action\": \"page_on_call\"},\n",
    "    \"latency\": {\"threshold\": 3000, \"action\": \"investigate\"},\n",
    "    \"cost\": {\"threshold\": 1000, \"action\": \"notify_team\"}\n",
    "}\n",
    "```\n",
    "\n",
    "### 5. Document Deployments\n",
    "\n",
    "**Maintain deployment registry:**\n",
    "\n",
    "```yaml\n",
    "# deployments.yaml\n",
    "deployments:\n",
    "  cora-v1.0:\n",
    "    model_id: ft:gpt-4o-mini:zava:abc123\n",
    "    deployed: 2024-11-01\n",
    "    status: production\n",
    "    capacity: 50000\n",
    "    notes: Initial fine-tuned model\n",
    "    \n",
    "  cora-v2.0:\n",
    "    model_id: ft:gpt-4o-mini:zava:xyz789\n",
    "    deployed: 2024-11-13\n",
    "    status: staging\n",
    "    capacity: 10000\n",
    "    notes: Improved product knowledge\n",
    "```\n",
    "\n",
    "**Track changes over time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6be8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Production Readiness Checklist\n",
    "\n",
    "Before deploying to production, verify:\n",
    "\n",
    "### âœ… Quality Validation\n",
    "\n",
    "- [ ] Evaluation metrics meet thresholds (groundedness > 4.0, etc.)\n",
    "- [ ] Safety checks pass (no high-severity content violations)\n",
    "- [ ] Tested on diverse query types\n",
    "- [ ] Edge cases handled gracefully\n",
    "- [ ] Regression tests pass (didn't break existing functionality)\n",
    "\n",
    "### âœ… Performance Testing\n",
    "\n",
    "- [ ] Latency < 3 seconds for 95th percentile\n",
    "- [ ] Load tested at expected traffic volume\n",
    "- [ ] Capacity sufficient for peak usage\n",
    "- [ ] No memory leaks or resource exhaustion\n",
    "- [ ] Graceful degradation under high load\n",
    "\n",
    "### âœ… Security & Compliance\n",
    "\n",
    "- [ ] Managed identity configured\n",
    "- [ ] Role-based access control (RBAC) set up\n",
    "- [ ] Content filtering enabled\n",
    "- [ ] No sensitive data in prompts/responses\n",
    "- [ ] Compliance requirements met (GDPR, etc.)\n",
    "\n",
    "### âœ… Monitoring & Observability\n",
    "\n",
    "- [ ] Application Insights configured\n",
    "- [ ] Logging enabled (errors, warnings)\n",
    "- [ ] Tracing set up (OpenTelemetry)\n",
    "- [ ] Alerts configured (errors, latency, cost)\n",
    "- [ ] Dashboard created for key metrics\n",
    "\n",
    "### âœ… Operational Readiness\n",
    "\n",
    "- [ ] Deployment runbook documented\n",
    "- [ ] Rollback procedure tested\n",
    "- [ ] On-call rotation established\n",
    "- [ ] Incident response plan ready\n",
    "- [ ] Capacity planning done (6-12 months)\n",
    "\n",
    "### âœ… Business Validation\n",
    "\n",
    "- [ ] Stakeholder approval obtained\n",
    "- [ ] Budget approved for production costs\n",
    "- [ ] User acceptance testing complete\n",
    "- [ ] Support team trained\n",
    "- [ ] Communication plan ready (users, customers)\n",
    "\n",
    "### Example Checklist in Code\n",
    "\n",
    "```python\n",
    "def is_production_ready(deployment):\n",
    "    \"\"\"Validate deployment meets production standards\"\"\"\n",
    "    checks = {\n",
    "        \"evaluation_passed\": check_evaluation_metrics(deployment),\n",
    "        \"performance_ok\": check_latency(deployment) < 3000,\n",
    "        \"security_configured\": check_rbac(deployment),\n",
    "        \"monitoring_enabled\": check_app_insights(deployment),\n",
    "        \"capacity_sufficient\": check_capacity(deployment) > expected_load,\n",
    "        \"rollback_tested\": check_rollback_works(deployment)\n",
    "    }\n",
    "    \n",
    "    all_passed = all(checks.values())\n",
    "    \n",
    "    if not all_passed:\n",
    "        failed = [k for k, v in checks.items() if not v]\n",
    "        print(f\"Failed checks: {failed}\")\n",
    "        \n",
    "    return all_passed\n",
    "\n",
    "if is_production_ready(\"cora-v2.0\"):\n",
    "    deploy_to_production(\"cora-v2.0\")\n",
    "else:\n",
    "    print(\"Not ready for production yet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17a731",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment Strategies\n",
    "\n",
    "### 1. Blue-Green Deployment\n",
    "\n",
    "**Maintain two identical environments:**\n",
    "\n",
    "```\n",
    "Blue (current production):\n",
    "- Deployment: cora-blue\n",
    "- Handles 100% traffic\n",
    "- Known stable version\n",
    "\n",
    "Green (new version):\n",
    "- Deployment: cora-green  \n",
    "- Handles 0% traffic initially\n",
    "- New version being validated\n",
    "\n",
    "Cutover:\n",
    "1. Deploy new version to Green\n",
    "2. Test Green thoroughly\n",
    "3. Switch traffic: Blue â†’ Green\n",
    "4. Keep Blue as rollback option\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Zero-downtime deployment\n",
    "- Instant rollback (switch back to Blue)\n",
    "- Full testing before production traffic\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "# Traffic router\n",
    "def route_traffic(use_green=False):\n",
    "    if use_green:\n",
    "        return \"cora-green\"\n",
    "    else:\n",
    "        return \"cora-blue\"\n",
    "\n",
    "# Gradual cutover\n",
    "MODEL = route_traffic(use_green=True)\n",
    "```\n",
    "\n",
    "### 2. Canary Deployment\n",
    "\n",
    "**Gradually roll out to small % of users:**\n",
    "\n",
    "```\n",
    "Phase 1: 5% traffic to new version\n",
    "  â†’ Monitor for 24 hours\n",
    "  â†’ Check metrics\n",
    "  \n",
    "Phase 2: 25% traffic to new version\n",
    "  â†’ Monitor for 24 hours\n",
    "  â†’ Check metrics\n",
    "  \n",
    "Phase 3: 50% traffic to new version\n",
    "  â†’ Monitor for 24 hours\n",
    "  â†’ Check metrics\n",
    "  \n",
    "Phase 4: 100% traffic to new version\n",
    "  â†’ Full rollout complete\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def get_deployment():\n",
    "    \"\"\"Route traffic based on rollout percentage\"\"\"\n",
    "    rollout_percentage = 0.25  # 25% to new version\n",
    "    \n",
    "    if random.random() < rollout_percentage:\n",
    "        return \"cora-v2.0\"  # New\n",
    "    else:\n",
    "        return \"cora-v1.0\"  # Old\n",
    "\n",
    "MODEL = get_deployment()\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Limit blast radius of issues\n",
    "- Real-world validation\n",
    "- Easy rollback (reduce percentage)\n",
    "\n",
    "### 3. A/B Testing\n",
    "\n",
    "**Compare two versions with real users:**\n",
    "\n",
    "```python\n",
    "def assign_model(user_id):\n",
    "    \"\"\"Consistent assignment per user\"\"\"\n",
    "    # Hash user ID to ensure same user always gets same version\n",
    "    if hash(user_id) % 2 == 0:\n",
    "        return \"cora-v1.0\"  # Control (A)\n",
    "    else:\n",
    "        return \"cora-v2.0\"  # Treatment (B)\n",
    "\n",
    "# Track metrics per version\n",
    "metrics = {\n",
    "    \"cora-v1.0\": {\"satisfaction\": [], \"latency\": []},\n",
    "    \"cora-v2.0\": {\"satisfaction\": [], \"latency\": []}\n",
    "}\n",
    "\n",
    "# After 1 week, compare\n",
    "analyze_ab_test(metrics)\n",
    "# â†’ If v2.0 is better, roll out to everyone\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Data-driven decisions\n",
    "- Measure real impact\n",
    "- Optimize for business metrics (satisfaction, conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397d440",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Monitoring and Scaling\n",
    "\n",
    "### Key Metrics to Monitor\n",
    "\n",
    "**1. Usage Metrics**\n",
    "\n",
    "```python\n",
    "# Track over time\n",
    "daily_queries = count_requests(period=\"day\")\n",
    "tokens_consumed = sum_tokens(period=\"day\")\n",
    "unique_users = count_unique_users(period=\"day\")\n",
    "\n",
    "# Compare to capacity\n",
    "utilization = tokens_consumed / total_capacity\n",
    "if utilization > 0.8:\n",
    "    alert(\"High utilization - consider scaling\")\n",
    "```\n",
    "\n",
    "**2. Performance Metrics**\n",
    "\n",
    "```python\n",
    "# Response times\n",
    "avg_latency = mean_response_time()\n",
    "p95_latency = percentile_95_response_time()\n",
    "p99_latency = percentile_99_response_time()\n",
    "\n",
    "# Targets\n",
    "assert avg_latency < 1500  # 1.5s average\n",
    "assert p95_latency < 3000  # 3s for 95% of requests\n",
    "```\n",
    "\n",
    "**3. Quality Metrics**\n",
    "\n",
    "```python\n",
    "# Sample responses daily\n",
    "daily_sample = random_sample(size=100)\n",
    "\n",
    "quality_scores = evaluate(\n",
    "    data=daily_sample,\n",
    "    evaluators=quality_evaluators\n",
    ")\n",
    "\n",
    "if quality_scores.average < threshold:\n",
    "    alert(\"Quality degradation detected\")\n",
    "```\n",
    "\n",
    "**4. Cost Metrics**\n",
    "\n",
    "```python\n",
    "# Track spending\n",
    "daily_cost = tokens_used * price_per_token\n",
    "monthly_projection = daily_cost * 30\n",
    "\n",
    "if monthly_projection > budget:\n",
    "    alert(f\"Projected to exceed budget: ${monthly_projection}\")\n",
    "```\n",
    "\n",
    "### Scaling Strategies\n",
    "\n",
    "**Vertical Scaling (increase capacity):**\n",
    "\n",
    "```python\n",
    "# Current: 10K TPM\n",
    "# Need: 25K TPM\n",
    "\n",
    "update_deployment(\n",
    "    name=\"cora-prod\",\n",
    "    capacity=25_000  # Scale up\n",
    ")\n",
    "```\n",
    "\n",
    "**Horizontal Scaling (multiple deployments):**\n",
    "\n",
    "```python\n",
    "# Create multiple deployments\n",
    "deployments = [\n",
    "    \"cora-prod-1\",  # 50K TPM\n",
    "    \"cora-prod-2\",  # 50K TPM\n",
    "    \"cora-prod-3\"   # 50K TPM\n",
    "]\n",
    "# Total: 150K TPM\n",
    "\n",
    "# Load balance across them\n",
    "def route_query(query):\n",
    "    deployment = random.choice(deployments)\n",
    "    return query_model(deployment, query)\n",
    "```\n",
    "\n",
    "**Auto-scaling (planned):**\n",
    "\n",
    "```python\n",
    "# Define scaling rules\n",
    "scaling_policy = {\n",
    "    \"metric\": \"utilization\",\n",
    "    \"scale_up\": {\n",
    "        \"threshold\": 0.8,  # 80% utilization\n",
    "        \"action\": \"increase_capacity_by_50_percent\"\n",
    "    },\n",
    "    \"scale_down\": {\n",
    "        \"threshold\": 0.3,  # 30% utilization\n",
    "        \"action\": \"decrease_capacity_by_25_percent\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Alerting\n",
    "\n",
    "**Set up proactive alerts:**\n",
    "\n",
    "```python\n",
    "alerts = [\n",
    "    {\n",
    "        \"name\": \"High Error Rate\",\n",
    "        \"condition\": \"error_rate > 0.05\",\n",
    "        \"severity\": \"critical\",\n",
    "        \"action\": \"page_on_call\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"High Latency\",\n",
    "        \"condition\": \"p95_latency > 3000ms\",\n",
    "        \"severity\": \"warning\",\n",
    "        \"action\": \"notify_team\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"High Cost\",\n",
    "        \"condition\": \"daily_cost > budget * 1.5\",\n",
    "        \"severity\": \"warning\",\n",
    "        \"action\": \"notify_finance\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Quality Degradation\",\n",
    "        \"condition\": \"groundedness < 4.0\",\n",
    "        \"severity\": \"warning\",\n",
    "        \"action\": \"investigate\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237e4cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Deployment Issues\n",
    "\n",
    "### Issue 1: Deployment Not Found\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: The deployment 'cora-v1' was not found\n",
    "```\n",
    "\n",
    "**Causes:**\n",
    "- Typo in deployment name\n",
    "- Deployment in different region\n",
    "- Deployment deleted\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "# List all deployments\n",
    "deployments = ml_client.online_deployments.list()\n",
    "for d in deployments:\n",
    "    print(d.name)\n",
    "\n",
    "# Verify exact name\n",
    "deployment_name = \"cora-fine-tuned-v1\"  # Use exact name\n",
    "```\n",
    "\n",
    "### Issue 2: Insufficient Capacity\n",
    "\n",
    "**Error:**\n",
    "```\n",
    "Error: Rate limit exceeded (429)\n",
    "```\n",
    "\n",
    "**Cause:** Too many requests for deployment capacity\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "```python\n",
    "# Option 1: Increase capacity\n",
    "update_deployment(capacity=50_000)\n",
    "\n",
    "# Option 2: Implement retry logic\n",
    "import time\n",
    "\n",
    "def query_with_retry(prompt, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.query(prompt)\n",
    "        except RateLimitError:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Option 3: Load balance across deployments\n",
    "```\n",
    "\n",
    "### Issue 3: High Latency\n",
    "\n",
    "**Symptom:** Responses take > 5 seconds\n",
    "\n",
    "**Causes:**\n",
    "- Overloaded deployment\n",
    "- Complex queries\n",
    "- Network issues\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "```python\n",
    "# 1. Check utilization\n",
    "if utilization > 0.9:\n",
    "    scale_up_deployment()\n",
    "\n",
    "# 2. Optimize prompts (reduce tokens)\n",
    "prompt = shorten_prompt(original_prompt)\n",
    "\n",
    "# 3. Use faster model for simple queries\n",
    "if is_simple_query(query):\n",
    "    model = \"gpt-4o-mini\"  # Faster\n",
    "else:\n",
    "    model = \"cora-fine-tuned\"  # Better quality\n",
    "```\n",
    "\n",
    "### Issue 4: Quality Regression\n",
    "\n",
    "**Symptom:** Deployed model performs worse than expected\n",
    "\n",
    "**Causes:**\n",
    "- Different data distribution in production\n",
    "- Fine-tuning overfitting\n",
    "- Configuration issues\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "```python\n",
    "# 1. Evaluate on production sample\n",
    "prod_sample = sample_production_queries(size=100)\n",
    "results = evaluate(data=prod_sample, evaluators=evaluators)\n",
    "\n",
    "# 2. Compare to development results\n",
    "if results.score < dev_results.score - 0.5:\n",
    "    alert(\"Quality regression detected\")\n",
    "    \n",
    "# 3. Retrain with production-like data\n",
    "new_training_data = augment_with_production_examples()\n",
    "fine_tune_again(data=new_training_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8d3b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Terminology Quick Reference\n",
    "\n",
    "| Term | Simple Definition |\n",
    "|------|-------------------|\n",
    "| **Deployment** | Making a model available for production use via API |\n",
    "| **Deployment Name** | Friendly name to reference deployed model |\n",
    "| **Model ID** | Unique identifier from fine-tuning job |\n",
    "| **Capacity (TPM)** | Tokens per minute the deployment can handle |\n",
    "| **Blue-Green Deployment** | Two identical environments for zero-downtime updates |\n",
    "| **Canary Deployment** | Gradual rollout to small percentage of users |\n",
    "| **A/B Testing** | Comparing two versions with real user traffic |\n",
    "| **Rollback** | Reverting to previous version if issues occur |\n",
    "| **Vertical Scaling** | Increasing capacity of single deployment |\n",
    "| **Horizontal Scaling** | Adding more deployments |\n",
    "| **Utilization** | Percentage of capacity being used |\n",
    "| **Latency** | Time to generate a response |\n",
    "| **Production** | Live environment serving real customers |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a599565",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Now that you understand deployment concepts, you're ready to deploy your fine-tuned model!\n",
    "\n",
    "### Hands-On Notebook in This Section\n",
    "\n",
    "- **`60-deployment.ipynb`** - Deploy your fine-tuned model to production\n",
    "  - List completed fine-tuning jobs\n",
    "  - Select model for deployment\n",
    "  - Create deployment with appropriate capacity\n",
    "  - Verify deployment is working\n",
    "  - Test deployed model\n",
    "  - Update application configuration\n",
    "\n",
    "### Recommended Learning Path\n",
    "\n",
    "1. **Start here** â†’ Understand concepts (this notebook)\n",
    "2. **Next** â†’ Deploy model (`60-deployment.ipynb`)\n",
    "3. **Then** â†’ Monitor production (Azure AI Foundry portal)\n",
    "4. **Advanced** â†’ Implement canary deployment\n",
    "5. **Continuous** â†’ Monitor metrics and scale as needed\n",
    "\n",
    "### After Deployment\n",
    "\n",
    "**Continue learning:**\n",
    "- **Monitoring** â†’ Set up Application Insights dashboards\n",
    "- **Tracing** â†’ Implement OpenTelemetry for observability\n",
    "- **Optimization** â†’ Fine-tune capacity based on usage\n",
    "- **Updates** â†’ Deploy new versions using best practices\n",
    "- **Scaling** â†’ Handle growth in user traffic\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "For deeper understanding:\n",
    "\n",
    "- **[Deploy Fine-Tuned Models](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython-new&pivots=programming-language-python#deploy-a-fine-tuned-model)** - Azure OpenAI deployment guide\n",
    "- **[Azure AI Foundry Deployment](https://learn.microsoft.com/azure/ai-studio/how-to/deploy-models-openai)** - Deployment in AI Foundry\n",
    "- **[Production Best Practices](https://learn.microsoft.com/azure/ai-services/openai/how-to/production-readiness)** - Production readiness checklist\n",
    "- **[Monitoring and Logging](https://learn.microsoft.com/azure/ai-services/openai/how-to/monitoring)** - Application Insights integration\n",
    "- **[Cost Management](https://learn.microsoft.com/azure/ai-services/openai/how-to/manage-costs)** - Optimizing deployment costs\n",
    "\n",
    "---\n",
    "\n",
    "Ready to deploy your model? Open `60-deployment.ipynb` to get started! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
