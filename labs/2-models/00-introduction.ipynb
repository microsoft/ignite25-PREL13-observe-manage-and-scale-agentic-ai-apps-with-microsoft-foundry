{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8777393",
   "metadata": {},
   "source": [
    "# üéØ Let's Talk Model Selection & Synthetic Data\n",
    "\n",
    "## üõí The Zava Scenario\n",
    "\n",
    "Cora needs to recommend home improvement products from Zava's catalog effectively. But which AI model should power Cora? Should we use GPT-4.1 for its advanced reasoning, or GPT-4o-mini for cost efficiency?\n",
    "\n",
    "**The Challenge**: Before deploying Cora to production, we need to evaluate different models to find the best balance of quality, speed, and cost. We also need realistic test data to compare how each model performs with customer queries.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this section, you'll understand:\n",
    "\n",
    "1. **How to choose between different AI models** for your use case\n",
    "2. **What synthetic datasets are** and why they're valuable for testing\n",
    "3. **How to generate query-response pairs** for evaluation\n",
    "4. **What RAG (Retrieval-Augmented Generation) is** and how it improves responses\n",
    "5. **Key evaluation metrics** to compare model performance\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Selecting the right model is critical because it impacts:\n",
    "- **Response quality** - How accurate and helpful Cora's recommendations are\n",
    "- **Cost** - Different models have different pricing structures\n",
    "- **Latency** - How quickly Cora responds to customers\n",
    "- **Scalability** - Whether the solution can handle production traffic\n",
    "\n",
    "Let's explore how to make informed model decisions using synthetic data and evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea6a2b1",
   "metadata": {},
   "source": [
    "## Why Model Selection Matters\n",
    "\n",
    "Not all language models are created equal. Different models have different strengths:\n",
    "\n",
    "| Model | Best For | Trade-offs |\n",
    "|-------|----------|------------|\n",
    "| **GPT-4o** | Complex reasoning, multimodal tasks | Higher cost, slower |\n",
    "| **GPT-4o-mini** | High-volume, speed-critical applications | Less nuanced, faster |\n",
    "| **GPT-4.1** | General-purpose, strong performance | Balanced cost/quality |\n",
    "| **Fine-tuned models** | Domain-specific tasks, consistent style | Requires training data |\n",
    "\n",
    "**Key factors in model selection:**\n",
    "\n",
    "### 1. Task Complexity\n",
    "- **Simple Q&A**: GPT-4o-mini works great\n",
    "- **Multi-step reasoning**: GPT-4o provides better results\n",
    "- **Domain expertise**: Consider fine-tuning\n",
    "\n",
    "### 2. Cost Constraints\n",
    "Models charge per token (input + output):\n",
    "- GPT-4o-mini: Lower per-token cost, higher volume capability\n",
    "- GPT-4o: Higher per-token cost, better quality\n",
    "\n",
    "**Example calculation:**\n",
    "```\n",
    "10,000 customer queries/day\n",
    "Average 500 tokens per conversation\n",
    "\n",
    "Model A (smaller): Lower cost per token\n",
    "Model B (larger):  Higher cost per token\n",
    "\n",
    "Choosing the right model can lead to significant cost savings at scale.\n",
    "\n",
    "Note: See Azure OpenAI Pricing for current rates:\n",
    "https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/\n",
    "```\n",
    "\n",
    "### 3. Latency Requirements\n",
    "- **Real-time chat**: GPT-4o-mini (faster responses)\n",
    "- **Background processing**: GPT-4o (quality over speed)\n",
    "\n",
    "### 4. Accuracy Requirements\n",
    "- **Factual precision critical** (e.g., medical, legal): GPT-4o + RAG\n",
    "- **General assistance** (e.g., recommendations): GPT-4o-mini sufficient\n",
    "\n",
    "\n",
    "**The Challenge:** How do you know which model is best for *your* use case?**The Solution:** Systematic evaluation with test datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885cc4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What are Synthetic Datasets?\n",
    "\n",
    "**Synthetic datasets** are artificially generated test data that simulate real-world scenarios.\n",
    "\n",
    "### Why Generate Synthetic Data?\n",
    "\n",
    "**Problem:** You need test data before you have real customer conversations.\n",
    "\n",
    "**Solutions:**\n",
    "1. ‚ùå Wait for real data ‚Üí Can't test until production\n",
    "2. ‚ùå Manually write test cases ‚Üí Time-consuming, limited coverage\n",
    "3. ‚úÖ Generate synthetic data ‚Üí Fast, scalable, covers edge cases\n",
    "\n",
    "### Benefits of Synthetic Data\n",
    "\n",
    "**1. Early Testing**\n",
    "- Test models before deployment\n",
    "- Catch issues in development\n",
    "- Iterate quickly on improvements\n",
    "\n",
    "**2. Comprehensive Coverage**\n",
    "- Generate hundreds of test cases quickly\n",
    "- Cover edge cases humans might miss\n",
    "- Test different customer intents and phrasings\n",
    "\n",
    "**3. Privacy & Safety**\n",
    "- No real customer data needed\n",
    "- Safe for development/testing\n",
    "- Compliance-friendly\n",
    "\n",
    "**4. Cost-Effective**\n",
    "- Faster than manual test creation\n",
    "- Cheaper than waiting for real data\n",
    "- Reusable across iterations\n",
    "\n",
    "### Example: Zava Product Questions\n",
    "\n",
    "**Real customer questions** (would take months to collect):\n",
    "```\n",
    "\"What paint do you have for exterior wood?\"\n",
    "\"I need a drill for concrete, what do you recommend?\"\n",
    "\"Do you have any eco-friendly paint options?\"\n",
    "\"What's the difference between latex and oil-based paint?\"\n",
    "```\n",
    "\n",
    "**Synthetic generation** (created in minutes):\n",
    "```python\n",
    "# Azure AI Simulator generates similar questions from product catalog\n",
    "simulator.generate_queries(\n",
    "    source=\"product_catalog\",\n",
    "    count=100,\n",
    "    variety=[\"product_search\", \"recommendations\", \"comparisons\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**Result:** 100 realistic customer questions in seconds, covering diverse intents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b983314",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Azure AI Evaluation Simulator\n",
    "\n",
    "The **Azure AI Evaluation Simulator** is a tool for generating synthetic query-response pairs based on your data sources.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Your Data Sources           Simulator              Synthetic Dataset\n",
    "                                ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Product Catalog ‚îÇ  ‚Üí   ‚îÇ  AI Simulator ‚îÇ  ‚Üí   ‚îÇ Query-Response   ‚îÇ\n",
    "‚îÇ Documentation   ‚îÇ      ‚îÇ               ‚îÇ      ‚îÇ Pairs (JSONL)    ‚îÇ\n",
    "‚îÇ FAQs            ‚îÇ      ‚îÇ  Uses LLM to  ‚îÇ      ‚îÇ                  ‚îÇ\n",
    "‚îÇ Knowledge Base  ‚îÇ      ‚îÇ  generate     ‚îÇ      ‚îÇ 100+ realistic   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ  realistic    ‚îÇ      ‚îÇ test examples    ‚îÇ\n",
    "                         ‚îÇ  questions     ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### What Gets Generated\n",
    "\n",
    "**Query-Response Pairs** in JSONL format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": \"What paint is best for exterior wood?\",\n",
    "  \"response\": \"For exterior wood, we recommend our Premium Exterior Paint (SKU: PFIP000002)...\",\n",
    "  \"context\": \"Product: Premium Exterior Paint, Category: Paint & Finishes\",\n",
    "  \"intent\": \"product_recommendation\"\n",
    "}\n",
    "```\n",
    "\n",
    "Each pair includes:\n",
    "- **Query**: A customer question\n",
    "- **Response**: Expected answer (generated from your data)\n",
    "- **Context**: Source information used\n",
    "- **Intent**: Type of question (optional metadata)\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**1. Data Source Connection**\n",
    "Connect to your knowledge base:\n",
    "```python\n",
    "# Example: Azure AI Search\n",
    "search_client = SearchClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    index_name=\"products\",\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_API_KEY)\n",
    ")\n",
    "```\n",
    "\n",
    "**2. RAG Application Callback**\n",
    "Define how to retrieve information:\n",
    "```python\n",
    "def query_product_info(query: str) -> str:\n",
    "    # Search product catalog\n",
    "    results = search_client.search(query, top=3)\n",
    "    # Format results\n",
    "    return formatted_results\n",
    "```\n",
    "\n",
    "**3. Simulation Configuration**\n",
    "Generate queries:\n",
    "```python\n",
    "simulator = Simulator(model_config=model_config)\n",
    "\n",
    "outputs = simulator.generate(\n",
    "    target=query_product_info,  # Your retrieval function\n",
    "    num_queries=100,            # How many to generate\n",
    "    max_conversation_turns=1    # Single Q&A or multi-turn\n",
    ")\n",
    "```\n",
    "\n",
    "### Output Format: JSONL\n",
    "\n",
    "**JSONL** (JSON Lines) = One JSON object per line\n",
    "\n",
    "```jsonl\n",
    "{\"query\": \"What drill bits do you have?\", \"response\": \"We offer...\"}\n",
    "{\"query\": \"Best paint for kitchens?\", \"response\": \"For kitchens...\"}\n",
    "{\"query\": \"Do you have eco-friendly products?\", \"response\": \"Yes, we have...\"}\n",
    "```\n",
    "\n",
    "**Why JSONL?**\n",
    "- Easy to stream and process line-by-line\n",
    "- Standard format for ML/AI tools\n",
    "- Efficient for large datasets\n",
    "- Compatible with evaluation libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f18b48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG: Retrieval-Augmented Generation\n",
    "\n",
    "**RAG** is a technique that improves AI responses by retrieving relevant information before generating answers.\n",
    "\n",
    "### The Problem Without RAG\n",
    "\n",
    "**Scenario:** Customer asks \"What is SKU PFIP000002?\"\n",
    "\n",
    "**Without RAG (model alone):**\n",
    "```\n",
    "Model: \"I don't have information about specific SKUs in my training data.\"\n",
    "```\n",
    "\n",
    "The model doesn't know your specific products.\n",
    "\n",
    "### The Solution: RAG\n",
    "\n",
    "**With RAG:**\n",
    "```\n",
    "1. Retrieve: Search product database for \"PFIP000002\"\n",
    "   ‚Üí Found: \"Premium Exterior Paint, $45.99, In Stock\"\n",
    "   \n",
    "2. Augment: Add retrieved info to prompt\n",
    "   \"Based on this product info: [Premium Exterior Paint...], answer the question\"\n",
    "   \n",
    "3. Generate: Model creates response\n",
    "   ‚Üí \"SKU PFIP000002 is Premium Exterior Paint, priced at $45.99 and currently in stock.\"\n",
    "```\n",
    "\n",
    "**Result:** Accurate, factual response grounded in real data.\n",
    "\n",
    "### How RAG Works\n",
    "\n",
    "```\n",
    "Customer Question\n",
    "      ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  1. Retrieve    ‚îÇ  Search knowledge base\n",
    "‚îÇ                 ‚îÇ  (Azure AI Search, Vector DB, etc.)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "   Retrieved Context\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  2. Augment     ‚îÇ  Combine question + context in prompt\n",
    "‚îÇ                 ‚îÇ  \"Based on: [context], answer: [question]\"\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "   Enhanced Prompt\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  3. Generate    ‚îÇ  LLM creates response using context\n",
    "‚îÇ                 ‚îÇ  \n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "   Factual Response\n",
    "```\n",
    "\n",
    "### RAG vs. Fine-Tuning vs. Prompting\n",
    "\n",
    "| Approach | Use When | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **RAG** | Data changes frequently | Always up-to-date, factual | Requires search infrastructure |\n",
    "| **Fine-Tuning** | Style/tone consistency needed | Efficient, no retrieval needed | Static knowledge, requires retraining |\n",
    "| **Prompting** | Simple tasks, static info | Fast, no infrastructure | Limited by context window |\n",
    "\n",
    "**For Cora:** Use RAG for product info (changes frequently) + fine-tuning for tone (static style).\n",
    "\n",
    "### Benefits for Model Evaluation\n",
    "\n",
    "When generating synthetic datasets with the simulator:\n",
    "- **RAG ensures grounded responses** - Answers based on real product data\n",
    "- **Realistic test cases** - Questions reflect actual product catalog\n",
    "- **Measurable accuracy** - Can verify responses against source data\n",
    "\n",
    "This is why the simulator uses a RAG callback - it generates test data that's realistic and verifiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102d963",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Concepts: Queries, Responses, and Pairs\n",
    "\n",
    "### Query\n",
    "\n",
    "A **query** is a question or request from the user.\n",
    "\n",
    "**Examples:**\n",
    "- \"What paint do you have for exterior wood?\"\n",
    "- \"I need a drill, what do you recommend?\"\n",
    "- \"Is SKU PFIP000002 in stock?\"\n",
    "\n",
    "**Query characteristics:**\n",
    "- **Intent**: What the user wants (search, recommendation, fact check)\n",
    "- **Complexity**: Simple vs. multi-part questions\n",
    "- **Specificity**: Broad (\"what paint?\") vs. specific (\"is PFIP000002 available?\")\n",
    "\n",
    "### Response\n",
    "\n",
    "A **response** is the answer generated by the AI model.\n",
    "\n",
    "**Examples:**\n",
    "```\n",
    "Query: \"What paint do you have for exterior wood?\"\n",
    "\n",
    "Response: \"For exterior wood projects, I recommend our Premium Exterior Paint \n",
    "(SKU: PFIP000002). It's weather-resistant, durable, and available in multiple \n",
    "colors. Currently priced at $45.99 with 75 units in stock.\"\n",
    "```\n",
    "\n",
    "**Quality factors:**\n",
    "- **Accuracy**: Factually correct\n",
    "- **Completeness**: Answers the full question\n",
    "- **Relevance**: Stays on topic\n",
    "- **Helpfulness**: Provides useful information\n",
    "- **Tone**: Matches brand voice (polite, professional)\n",
    "\n",
    "### Query-Response Pair\n",
    "\n",
    "A **pair** combines a query with its expected response for testing.\n",
    "\n",
    "**Format:**\n",
    "```json\n",
    "{\n",
    "  \"query\": \"What paint is best for exterior wood?\",\n",
    "  \"response\": \"For exterior wood, I recommend Premium Exterior Paint (SKU: PFIP000002)...\",\n",
    "  \"ground_truth\": \"Premium Exterior Paint\",\n",
    "  \"context\": \"Products: PFIP000002, PFIP000003, PFIP000005\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Why pairs?**\n",
    "- **Baseline for comparison**: Expected vs. actual responses\n",
    "- **Reproducible testing**: Same queries across model versions\n",
    "- **Quality metrics**: Measure how well responses match expectations\n",
    "\n",
    "### Evaluation Workflow\n",
    "\n",
    "```\n",
    "1. Generate Pairs (Simulator)\n",
    "   ‚Üí 100 query-response pairs from product catalog\n",
    "\n",
    "2. Test Model A (GPT-4o-mini)\n",
    "   ‚Üí Run 100 queries through model\n",
    "   ‚Üí Collect 100 responses\n",
    "\n",
    "3. Test Model B (GPT-4o)\n",
    "   ‚Üí Run same 100 queries through different model\n",
    "   ‚Üí Collect 100 responses\n",
    "\n",
    "4. Compare Results\n",
    "   ‚Üí Which model's responses better match expected responses?\n",
    "   ‚Üí Which is more accurate, helpful, relevant?\n",
    "\n",
    "5. Select Winner\n",
    "   ‚Üí Choose model based on metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290651cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation Metrics for Model Selection\n",
    "\n",
    "How do you measure which model is better? Use these metrics:\n",
    "\n",
    "### 1. Groundedness\n",
    "\n",
    "**What it measures:** Are responses based on retrieved context (not hallucinated)?\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Context: \"Premium Exterior Paint costs $45.99\"\n",
    "\n",
    "Good (grounded): \"Premium Exterior Paint is priced at $45.99\"\n",
    "Bad (not grounded): \"Premium Exterior Paint costs around $40\"\n",
    "```\n",
    "\n",
    "**Score:** 0 to 5 (5 = fully grounded in context)\n",
    "\n",
    "### 2. Relevance\n",
    "\n",
    "**What it measures:** Does the response address the query?\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"What paint is best for exterior wood?\"\n",
    "\n",
    "Good (relevant): \"For exterior wood, Premium Exterior Paint is ideal...\"\n",
    "Bad (irrelevant): \"We have many paint options in different colors...\"\n",
    "```\n",
    "\n",
    "**Score:** 0 to 5 (5 = perfectly relevant)\n",
    "\n",
    "### 3. Coherence\n",
    "\n",
    "**What it measures:** Is the response well-structured and logical?\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Good (coherent): \"Premium Exterior Paint is durable and weather-resistant, \n",
    "making it perfect for outdoor wood surfaces.\"\n",
    "\n",
    "Bad (incoherent): \"Paint wood exterior durable Premium weather yes outdoor.\"\n",
    "```\n",
    "\n",
    "**Score:** 0 to 5 (5 = perfectly coherent)\n",
    "\n",
    "### 4. Fluency\n",
    "\n",
    "**What it measures:** Is the language natural and grammatically correct?\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Good (fluent): \"I recommend Premium Exterior Paint for your project.\"\n",
    "Bad (not fluent): \"I recommend you Premium Exterior Paint is for project.\"\n",
    "```\n",
    "\n",
    "**Score:** 0 to 5 (5 = perfect grammar and naturalness)\n",
    "\n",
    "### 5. Similarity (to Expected Response)\n",
    "\n",
    "**What it measures:** How close is the actual response to the expected response?\n",
    "\n",
    "**Measured by:**\n",
    "- Cosine similarity (embedding vectors)\n",
    "- BLEU score (text overlap)\n",
    "- Semantic similarity (meaning)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Expected: \"Premium Exterior Paint costs $45.99 and is in stock.\"\n",
    "Actual:   \"Our Premium Exterior Paint is priced at $45.99 with availability.\"\n",
    "\n",
    "Similarity: 0.92 (very similar)\n",
    "```\n",
    "\n",
    "### Combining Metrics\n",
    "\n",
    "**Model evaluation scorecard:**\n",
    "\n",
    "| Metric | GPT-4o-mini | GPT-4o | Winner |\n",
    "|--------|-------------|--------|--------|\n",
    "| Groundedness | 4.2 | 4.8 | GPT-4o |\n",
    "| Relevance | 4.5 | 4.7 | GPT-4o |\n",
    "| Coherence | 4.3 | 4.6 | GPT-4o |\n",
    "| Fluency | 4.6 | 4.7 | GPT-4o |\n",
    "| Similarity | 0.85 | 0.91 | GPT-4o |\n",
    "| **Avg Latency** | **800ms** | **1200ms** | **GPT-4o-mini** |\n",
    "| **Cost/1K queries** | **Lower** | **Higher** | **GPT-4o-mini** |\n",
    "\n",
    "**Note:** For current pricing, see [Azure OpenAI Pricing](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/)\n",
    "\n",
    "**Decision:** \n",
    "- If quality is critical ‚Üí Choose GPT-4o\n",
    "\n",
    "- If speed/cost is critical ‚Üí Choose GPT-4o-mini- Hybrid: Use GPT-4o-mini for simple queries, GPT-4o for complex ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3ae1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Model Selection Process\n",
    "\n",
    "### Step 1: Define Requirements\n",
    "\n",
    "**Ask yourself:**\n",
    "- What is my task? (Q&A, recommendations, complex reasoning)\n",
    "- What quality level is acceptable?\n",
    "- What's my budget?\n",
    "- What latency is acceptable?\n",
    "- How many queries per day?\n",
    "\n",
    "**Example for Cora:**\n",
    "- Task: Product Q&A, inventory checks\n",
    "- Quality: High accuracy required (factual info)\n",
    "- Budget: Moderate (thousands of queries/day)\n",
    "- Latency: < 2 seconds preferred\n",
    "- Volume: ~5,000 queries/day\n",
    "\n",
    "### Step 2: Generate Test Dataset\n",
    "\n",
    "**Use Azure AI Simulator:**\n",
    "```python\n",
    "# Generate 100 query-response pairs from product catalog\n",
    "dataset = simulator.generate(\n",
    "    target=rag_callback,\n",
    "    num_queries=100\n",
    ")\n",
    "\n",
    "# Save to JSONL\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "    for item in dataset:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "```\n",
    "\n",
    "**Result:** 100 realistic customer questions with expected answers\n",
    "\n",
    "### Step 3: Test Candidate Models\n",
    "\n",
    "**Run each model on the test dataset:**\n",
    "\n",
    "```python\n",
    "# Test GPT-4o-mini\n",
    "results_mini = []\n",
    "for item in test_dataset:\n",
    "    response = model_mini.query(item[\"query\"])\n",
    "    results_mini.append({\n",
    "        \"query\": item[\"query\"],\n",
    "        \"response\": response,\n",
    "        \"expected\": item[\"response\"]\n",
    "    })\n",
    "\n",
    "# Test GPT-4o\n",
    "results_4o = []\n",
    "for item in test_dataset:\n",
    "    response = model_4o.query(item[\"query\"])\n",
    "    results_4o.append({\n",
    "        \"query\": item[\"query\"],\n",
    "        \"response\": response,\n",
    "        \"expected\": item[\"response\"]\n",
    "    })\n",
    "```\n",
    "\n",
    "### Step 4: Evaluate Results\n",
    "\n",
    "**Use evaluation metrics:**\n",
    "\n",
    "```python\n",
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Evaluate GPT-4o-mini\n",
    "eval_mini = evaluate(\n",
    "    data=results_mini,\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"coherence\": coherence_evaluator\n",
    "    }\n",
    ")\n",
    "\n",
    "# Evaluate GPT-4o\n",
    "eval_4o = evaluate(\n",
    "    data=results_4o,\n",
    "    evaluators={...}\n",
    ")\n",
    "```\n",
    "\n",
    "### Step 5: Compare and Decide\n",
    "\n",
    "**Create comparison:**\n",
    "\n",
    "```python\n",
    "comparison = {\n",
    "    \"GPT-4o-mini\": {\n",
    "        \"quality\": eval_mini.average_scores,\n",
    "        \"cost\": 0.50,\n",
    "        \"latency\": 800\n",
    "    },\n",
    "    \"GPT-4o\": {\n",
    "        \"quality\": eval_4o.average_scores,\n",
    "        \"cost\": 2.50,\n",
    "        \"latency\": 1200\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Decision framework:**\n",
    "\n",
    "```\n",
    "If quality_difference < 0.3 AND cost_difference > 2x:\n",
    "    ‚Üí Choose cheaper model (GPT-4o-mini)\n",
    "    \n",
    "If quality_difference > 0.5:\n",
    "    ‚Üí Choose better model (GPT-4o)\n",
    "    \n",
    "Else:\n",
    "    ‚Üí Consider hybrid approach\n",
    "```\n",
    "\n",
    "### Step 6: Iterate\n",
    "\n",
    "- Test with more data points\n",
    "- Try different prompts\n",
    "- Consider fine-tuning\n",
    "- Re-evaluate periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0cf5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Generate Diverse Test Cases\n",
    "\n",
    "```python\n",
    "# Good - covers different intents\n",
    "queries = [\n",
    "    \"What paint is best for exterior wood?\",      # Recommendation\n",
    "    \"Is SKU PFIP000002 in stock?\",                # Fact check\n",
    "    \"Compare latex vs oil-based paint\",           # Comparison\n",
    "    \"I need eco-friendly options\",                # Filtered search\n",
    "    \"What's the price of Premium Exterior Paint?\" # Specific fact\n",
    "]\n",
    "\n",
    "# Less effective - repetitive\n",
    "queries = [\n",
    "    \"What paint do you have?\",\n",
    "    \"Do you sell paint?\",\n",
    "    \"Tell me about paint\",\n",
    "    # All similar intent\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. Use Realistic Phrasings\n",
    "\n",
    "Generate queries that match how real customers talk:\n",
    "\n",
    "```python\n",
    "# Realistic\n",
    "\"I'm painting my deck, what should I use?\"\n",
    "\"Need something for outdoor wood\"\n",
    "\"Best paint for weather resistance?\"\n",
    "\n",
    "# Too formal (less realistic)\n",
    "\"Please provide recommendations for exterior wood coating solutions\"\n",
    "```\n",
    "\n",
    "### 3. Include Edge Cases\n",
    "\n",
    "```python\n",
    "# Test edge cases\n",
    "\"Do you have paint?\" # Vague\n",
    "\"I need PFIP000002 but in blue\" # Specific constraint\n",
    "\"What's the cheapest paint?\" # Price-focused\n",
    "\"\" # Empty query\n",
    "\"ajshdkajhsd\" # Gibberish\n",
    "```\n",
    "\n",
    "### 4. Balance Dataset Size\n",
    "\n",
    "- **Too small** (< 20 queries): Not representative\n",
    "- **Good** (50-100 queries): Balanced coverage\n",
    "- **Large** (500+ queries): Comprehensive, but slower/costlier to run\n",
    "\n",
    "**Start with 50-100, expand if needed**\n",
    "\n",
    "### 5. Version Your Datasets\n",
    "\n",
    "```python\n",
    "# Save with version numbers\n",
    "\"test_dataset_v1.jsonl\"  # Initial\n",
    "\"test_dataset_v2.jsonl\"  # Added edge cases\n",
    "\"test_dataset_v3.jsonl\"  # Added multi-turn conversations\n",
    "```\n",
    "\n",
    "This lets you compare model performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eeb83b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Terminology Quick Reference\n",
    "\n",
    "| Term | Simple Definition |\n",
    "|------|-------------------|\n",
    "| **Synthetic Dataset** | Artificially generated test data simulating real scenarios |\n",
    "| **Query** | A question or request from the user |\n",
    "| **Response** | The answer generated by the AI model |\n",
    "| **Query-Response Pair** | A test case combining a query with expected response |\n",
    "| **RAG** | Retrieval-Augmented Generation - retrieve context before generating |\n",
    "| **JSONL** | JSON Lines format - one JSON object per line |\n",
    "| **Groundedness** | Metric measuring if response is based on provided context |\n",
    "| **Relevance** | Metric measuring if response addresses the query |\n",
    "| **Coherence** | Metric measuring if response is well-structured |\n",
    "| **Fluency** | Metric measuring if response has natural language quality |\n",
    "| **Similarity** | Metric comparing actual vs. expected responses |\n",
    "| **Simulator** | Tool that generates synthetic test data |\n",
    "| **Latency** | Time taken to generate a response |\n",
    "| **Token** | Unit of text (~4 characters) used for billing |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e205273",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Now that you understand model selection concepts, you're ready to generate and evaluate test datasets!\n",
    "\n",
    "### Hands-On Notebooks in This Section\n",
    "\n",
    "- **`21-simulate-dataset.ipynb`** - Generate synthetic test data with Azure AI Simulator\n",
    "  - Connect to Azure AI Search\n",
    "  - Create RAG application callback\n",
    "  - Generate query-response pairs\n",
    "  - Save datasets in JSONL format\n",
    "\n",
    "- **`22-evaluate-models.ipynb`** - Compare different models using your test dataset\n",
    "  - Run queries through multiple models\n",
    "  - Calculate evaluation metrics\n",
    "  - Compare quality, cost, and latency\n",
    "  - Make data-driven model selection\n",
    "\n",
    "### Recommended Learning Path\n",
    "\n",
    "1. **Start here** ‚Üí Understand concepts (this notebook)\n",
    "2. **Next** ‚Üí Generate test dataset (`21-simulate-dataset.ipynb`)\n",
    "3. **Then** ‚Üí Evaluate models (`22-evaluate-models.ipynb`)\n",
    "4. **After** ‚Üí Move to customization labs (fine-tuning, distillation)\n",
    "5. **Finally** ‚Üí Deploy and monitor your chosen model\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "For deeper understanding:\n",
    "\n",
    "- **[Azure AI Evaluation SDK](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk)** - Official evaluation guide\n",
    "- **[Azure AI Simulator](https://learn.microsoft.com/azure/ai-studio/how-to/develop/simulator-interaction-data)** - Generate synthetic datasets\n",
    "- **[RAG Overview](https://learn.microsoft.com/azure/ai-studio/concepts/retrieval-augmented-generation)** - Retrieval-Augmented Generation concepts\n",
    "- **[Model Selection Guide](https://learn.microsoft.com/azure/ai-services/openai/concepts/models)** - Choosing the right model\n",
    "- **[Evaluation Metrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in)** - Understanding quality metrics\n",
    "\n",
    "---\n",
    "\n",
    "Ready to generate test data? Open `21-simulate-dataset.ipynb` to get started! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
