{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcce8ba",
   "metadata": {},
   "source": [
    "# ðŸ“Š Introduction to AI Evaluation\n",
    "\n",
    "## ðŸ›’ The Zava Scenario\n",
    "\n",
    "Cora is now responding to customer queries about home improvement products, but how do we know if the responses are good? Are they accurate, relevant, and safe? Before deploying Cora to production, Zava needs objective metrics to measure quality and safety.\n",
    "\n",
    "**The Challenge**: Evaluating AI systems requires more than manual spot-checking. We need systematic, repeatable methods to assess response quality, ensure factual accuracy, and detect potential safety issues.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this section, you'll understand:\n",
    "\n",
    "1. **GenAIOps** - The complete lifecycle of generative AI operations\n",
    "2. **Quality metrics** - Groundedness, relevance, coherence, and fluency\n",
    "3. **Safety metrics** - Detecting harmful content and jailbreak attempts\n",
    "4. **How to use Azure AI Evaluation SDK** to run evaluations\n",
    "5. **Interpreting evaluation results** to improve your agent\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Rigorous evaluation helps you:\n",
    "- **Build confidence** before production deployment\n",
    "- **Compare models objectively** using standardized metrics\n",
    "- **Identify issues early** before customers encounter them\n",
    "- **Continuously improve** by tracking metrics over time\n",
    "- **Meet compliance requirements** for responsible AI deployment\n",
    "\n",
    "Let's explore how to systematically evaluate Cora's performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cde247",
   "metadata": {},
   "source": [
    "## Why AI Evaluation Matters\n",
    "\n",
    "**The Challenge:** How do you know if your AI application is good enough for production?\n",
    "\n",
    "### The Risk of Skipping Evaluation\n",
    "\n",
    "**Without evaluation:**\n",
    "- âŒ **Hallucinations** - Cora invents product information\n",
    "- âŒ **Irrelevant responses** - Answers don't address customer questions\n",
    "- âŒ **Unsafe content** - Generates harmful or biased responses\n",
    "- âŒ **Inconsistent quality** - Sometimes great, sometimes terrible\n",
    "- âŒ **No improvement path** - Can't measure if changes help\n",
    "\n",
    "**Example failure:**\n",
    "```\n",
    "Customer: \"Is SKU PFIP000002 in stock?\"\n",
    "Cora (hallucinating): \"Yes! We have 150 units available at $39.99\"\n",
    "Reality: Out of stock, costs $45.99\n",
    "\n",
    "Result: Customer frustration, lost trust, potential legal issues\n",
    "```\n",
    "\n",
    "### The Value of Evaluation\n",
    "\n",
    "**With systematic evaluation:**\n",
    "- âœ… **Quantify quality** - \"Accuracy is 92%, up from 85% last week\"\n",
    "- âœ… **Catch issues early** - Find problems before customers do\n",
    "- âœ… **Compare approaches** - \"Model A is 15% better than Model B\"\n",
    "- âœ… **Track improvements** - \"Fine-tuning increased relevance by 20%\"\n",
    "- âœ… **Build confidence** - Know your system is ready for production\n",
    "\n",
    "**Example success:**\n",
    "```\n",
    "Evaluation shows:\n",
    "- Groundedness: 4.8/5 (responses based on facts)\n",
    "- Relevance: 4.6/5 (addresses customer questions)\n",
    "- Safety: 5/5 (no harmful content)\n",
    "\n",
    "Conclusion: Ready for production deployment\n",
    "```\n",
    "\n",
    "### Evaluation vs. Testing\n",
    "\n",
    "**Traditional software testing:**\n",
    "```\n",
    "Input: \"2 + 2\"\n",
    "Expected output: \"4\"\n",
    "Test: Does output == \"4\"? â†’ Pass/Fail\n",
    "```\n",
    "\n",
    "**AI evaluation:**\n",
    "```\n",
    "Input: \"What paint is best for kitchens?\"\n",
    "Expected: Helpful, accurate, relevant response\n",
    "Evaluation: How good is the response? â†’ Score 0-5\n",
    "```\n",
    "\n",
    "**Key differences:**\n",
    "- Software: Binary (pass/fail)\n",
    "- AI: Continuous (scored on scale)\n",
    "- Software: Deterministic (same output every time)\n",
    "- AI: Non-deterministic (outputs vary)\n",
    "\n",
    "**This is why we need specialized evaluation methods for AI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd82e82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The GenAIOps Lifecycle\n",
    "\n",
    "**GenAIOps** (Generative AI Operations) is the practice of building, evaluating, deploying, and monitoring AI applications.\n",
    "\n",
    "### The Three Stages of Evaluation\n",
    "\n",
    "```\n",
    "Stage 1: Base Model Selection\n",
    "  â†“\n",
    "  Evaluate candidate models\n",
    "  Choose best model for task\n",
    "  \n",
    "Stage 2: Pre-Production Development  \n",
    "  â†“\n",
    "  Iterate on prototypes\n",
    "  Evaluate improvements\n",
    "  Validate edge cases\n",
    "  \n",
    "Stage 3: Post-Production Monitoring\n",
    "  â†“\n",
    "  Track performance in production\n",
    "  Detect quality degradation\n",
    "  Continuous evaluation\n",
    "```\n",
    "\n",
    "### Stage 1: Base Model Selection\n",
    "\n",
    "**Goal:** Choose the right foundation model\n",
    "\n",
    "**Questions to answer:**\n",
    "- Which model is most accurate for my task?\n",
    "- Which offers the best cost/quality balance?\n",
    "- Which handles my domain best?\n",
    "\n",
    "**Evaluation approach:**\n",
    "```python\n",
    "# Compare models on same test dataset\n",
    "models = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-4.1\"]\n",
    "\n",
    "for model in models:\n",
    "    results = evaluate(\n",
    "        model=model,\n",
    "        data=test_dataset,\n",
    "        evaluators=quality_evaluators\n",
    "    )\n",
    "    print(f\"{model}: {results.average_score}\")\n",
    "\n",
    "# Choose winner based on scores\n",
    "```\n",
    "\n",
    "**Outcome:** Select foundation model for development\n",
    "\n",
    "### Stage 2: Pre-Production Development\n",
    "\n",
    "**Goal:** Refine application to meet quality standards\n",
    "\n",
    "**Activities:**\n",
    "- Prompt engineering iterations\n",
    "- Fine-tuning and customization\n",
    "- RAG configuration\n",
    "- Agent tool selection\n",
    "\n",
    "**Evaluation approach:**\n",
    "```python\n",
    "# Baseline\n",
    "baseline_results = evaluate(version=\"v1\", data=test_data)\n",
    "\n",
    "# After improvements\n",
    "improved_results = evaluate(version=\"v2\", data=test_data)\n",
    "\n",
    "# Compare\n",
    "if improved_results.score > baseline_results.score:\n",
    "    print(\"Improvement validated!\")\n",
    "else:\n",
    "    print(\"Need to try different approach\")\n",
    "```\n",
    "\n",
    "**Outcome:** High-quality application ready for deployment\n",
    "\n",
    "### Stage 3: Post-Production Monitoring\n",
    "\n",
    "**Goal:** Maintain quality in production\n",
    "\n",
    "**Activities:**\n",
    "- Sample real user interactions\n",
    "- Detect quality degradation\n",
    "- Identify new failure patterns\n",
    "- Trigger re-evaluation when needed\n",
    "\n",
    "**Evaluation approach:**\n",
    "```python\n",
    "# Daily monitoring\n",
    "production_sample = get_random_sample(size=100)\n",
    "\n",
    "daily_results = evaluate(\n",
    "    data=production_sample,\n",
    "    evaluators=quality_evaluators\n",
    ")\n",
    "\n",
    "if daily_results.score < threshold:\n",
    "    alert_team(\"Quality degradation detected!\")\n",
    "```\n",
    "\n",
    "**Outcome:** Continuously reliable AI application\n",
    "\n",
    "### Where We Are in This Lab\n",
    "\n",
    "**Focus:** Stage 2 - Pre-Production Development\n",
    "\n",
    "You'll learn to:\n",
    "1. Run evaluations on test datasets\n",
    "2. Measure quality and safety metrics\n",
    "3. Compare different configurations\n",
    "4. Validate improvements before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206da39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding Evaluators\n",
    "\n",
    "**Evaluators** are functions that score different aspects of your AI's responses.\n",
    "\n",
    "### Types of Evaluators\n",
    "\n",
    "**1. Quality Evaluators**\n",
    "Measure response quality:\n",
    "- **Groundedness** - Based on provided context?\n",
    "- **Relevance** - Addresses the question?\n",
    "- **Coherence** - Logically structured?\n",
    "- **Fluency** - Natural language?\n",
    "\n",
    "**2. Safety Evaluators**\n",
    "Measure content safety:\n",
    "- **Hate/Unfairness** - Biased or discriminatory?\n",
    "- **Violence** - Promotes harm?\n",
    "- **Sexual Content** - Inappropriate content?\n",
    "- **Self-Harm** - Dangerous recommendations?\n",
    "\n",
    "**3. Custom Evaluators**\n",
    "Measure domain-specific requirements:\n",
    "- **Brand Voice** - Matches company tone?\n",
    "- **Factual Accuracy** - Correct product info?\n",
    "- **Compliance** - Meets regulatory requirements?\n",
    "\n",
    "### Built-In vs. Custom Evaluators\n",
    "\n",
    "**Built-in evaluators** (Azure AI provides):\n",
    "\n",
    "```python\n",
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    ViolenceEvaluator,\n",
    "    HateUnfairnessEvaluator\n",
    ")\n",
    "\n",
    "# Use out-of-the-box\n",
    "evaluators = {\n",
    "    \"groundedness\": GroundednessEvaluator(),\n",
    "    \"relevance\": RelevanceEvaluator(),\n",
    "    \"safety\": ViolenceEvaluator()\n",
    "}\n",
    "```\n",
    "\n",
    "**Custom evaluators** (you create):\n",
    "\n",
    "```python\n",
    "def brand_voice_evaluator(response: str) -> dict:\n",
    "    \"\"\"Check if response matches Zava's friendly, helpful tone\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Check for friendly greeting\n",
    "    if any(word in response.lower() for word in [\"great\", \"happy to\", \"glad\"]):\n",
    "        score += 1\n",
    "    \n",
    "    # Check for helpfulness\n",
    "    if \"let me help\" in response.lower() or \"i recommend\" in response.lower():\n",
    "        score += 1\n",
    "    \n",
    "    # Check for product details\n",
    "    if \"SKU\" in response or \"$\" in response:\n",
    "        score += 1\n",
    "    \n",
    "    return {\n",
    "        \"brand_voice_score\": score / 3,  # Normalize to 0-1\n",
    "        \"reasoning\": f\"Found {score}/3 brand voice elements\"\n",
    "    }\n",
    "```\n",
    "\n",
    "### How Evaluators Work\n",
    "\n",
    "**Process:**\n",
    "\n",
    "```\n",
    "1. Evaluator receives inputs:\n",
    "   - Query: \"What paint do you have?\"\n",
    "   - Response: \"We have Premium Exterior Paint...\"\n",
    "   - Context: [retrieved product information]\n",
    "\n",
    "2. Evaluator analyzes response:\n",
    "   - Uses rules, patterns, or AI models\n",
    "   - Compares response to expectations\n",
    "   \n",
    "3. Evaluator returns score:\n",
    "   - Numeric score (e.g., 0-5 or 0-1)\n",
    "   - Reasoning (why this score)\n",
    "```\n",
    "\n",
    "**Example: Groundedness Evaluator**\n",
    "\n",
    "```python\n",
    "def groundedness_evaluator(query, response, context):\n",
    "    \"\"\"Check if response is grounded in context\"\"\"\n",
    "    \n",
    "    # Extract claims from response\n",
    "    claims = extract_claims(response)\n",
    "    \n",
    "    # Verify each claim against context\n",
    "    grounded_claims = 0\n",
    "    for claim in claims:\n",
    "        if verify_claim_in_context(claim, context):\n",
    "            grounded_claims += 1\n",
    "    \n",
    "    # Calculate score\n",
    "    score = grounded_claims / len(claims) if claims else 0\n",
    "    \n",
    "    return {\n",
    "        \"groundedness_score\": score,\n",
    "        \"reasoning\": f\"{grounded_claims}/{len(claims)} claims grounded\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9124a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Quality Metrics\n",
    "\n",
    "### 1. Groundedness\n",
    "\n",
    "**What it measures:** Are responses based on provided context (not hallucinated)?\n",
    "\n",
    "**Scale:** 1-5\n",
    "- **5** - All claims supported by context\n",
    "- **3** - Most claims supported, some unsupported\n",
    "- **1** - Response contradicts or ignores context\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Context: \"Premium Exterior Paint (PFIP000002) costs $45.99, in stock: 75 units\"\n",
    "\n",
    "Query: \"How much is PFIP000002?\"\n",
    "\n",
    "Good (Score: 5):\n",
    "\"Premium Exterior Paint (PFIP000002) is priced at $45.99\"\n",
    "â†’ Directly from context\n",
    "\n",
    "Bad (Score: 1):  \n",
    "\"PFIP000002 costs about $40, we have hundreds in stock\"\n",
    "â†’ Hallucinated numbers\n",
    "```\n",
    "\n",
    "**Why it matters:** Prevents misinformation and hallucinations\n",
    "\n",
    "### 2. Relevance\n",
    "\n",
    "**What it measures:** Does the response address the user's question?\n",
    "\n",
    "**Scale:** 1-5\n",
    "- **5** - Directly answers the question\n",
    "- **3** - Partially relevant, some tangents\n",
    "- **1** - Unrelated to question\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Query: \"What paint is best for exterior wood?\"\n",
    "\n",
    "Good (Score: 5):\n",
    "\"For exterior wood, I recommend Premium Exterior Paint (PFIP000002). \n",
    "It's weather-resistant and designed for outdoor applications.\"\n",
    "â†’ Directly addresses exterior wood painting\n",
    "\n",
    "Bad (Score: 2):\n",
    "\"We have many paint options in various colors. Our store hours are \n",
    "Monday-Friday 9am-6pm.\"\n",
    "â†’ Doesn't answer the question\n",
    "```\n",
    "\n",
    "**Why it matters:** Ensures responses are helpful and on-topic\n",
    "\n",
    "### 3. Coherence\n",
    "\n",
    "**What it measures:** Is the response logically structured and easy to follow?\n",
    "\n",
    "**Scale:** 1-5\n",
    "- **5** - Perfect logical flow, well-organized\n",
    "- **3** - Understandable but somewhat disorganized\n",
    "- **1** - Confusing, contradictory, or illogical\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Query: \"Why should I choose Premium Exterior Paint?\"\n",
    "\n",
    "Good (Score: 5):\n",
    "\"Premium Exterior Paint is ideal for outdoor projects because it offers:\n",
    "1. Weather resistance for durability\n",
    "2. UV protection to prevent fading  \n",
    "3. Easy application for DIY projects\n",
    "These features make it perfect for exterior wood surfaces.\"\n",
    "â†’ Clear structure, logical flow\n",
    "\n",
    "Bad (Score: 2):\n",
    "\"Paint exterior weather Premium also colors many wood protection \n",
    "outdoor durability yes easy.\"\n",
    "â†’ Incoherent, hard to understand\n",
    "```\n",
    "\n",
    "**Why it matters:** Readability and user comprehension\n",
    "\n",
    "### 4. Fluency\n",
    "\n",
    "**What it measures:** Is the language natural and grammatically correct?\n",
    "\n",
    "**Scale:** 1-5\n",
    "- **5** - Perfect grammar, natural phrasing\n",
    "- **3** - Minor errors, slightly awkward\n",
    "- **1** - Many errors, unnatural language\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Query: \"Do you have eco-friendly paint?\"\n",
    "\n",
    "Good (Score: 5):\n",
    "\"Yes! We offer several eco-friendly paint options with low VOC \n",
    "formulas that are safer for your family and the environment.\"\n",
    "â†’ Natural, grammatically perfect\n",
    "\n",
    "Bad (Score: 2):\n",
    "\"Yes eco friendly paint we are having low VOC is safer for you \n",
    "family and environment also.\"\n",
    "â†’ Grammatical errors, awkward phrasing\n",
    "```\n",
    "\n",
    "**Why it matters:** Professional communication and user trust\n",
    "\n",
    "### 5. Similarity\n",
    "\n",
    "**What it measures:** How close is the response to an expected/reference answer?\n",
    "\n",
    "**Scale:** 0-1 (cosine similarity)\n",
    "- **0.9+** - Very similar\n",
    "- **0.7-0.9** - Similar meaning\n",
    "- **< 0.7** - Different\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Expected: \"Premium Exterior Paint costs $45.99 and is in stock\"\n",
    "Actual: \"PFIP000002 is priced at $45.99 with availability\"\n",
    "\n",
    "Similarity: 0.92 (very similar)\n",
    "```\n",
    "\n",
    "**How it's calculated:**\n",
    "1. Convert both texts to embeddings (vectors)\n",
    "2. Calculate cosine similarity between vectors\n",
    "3. Higher value = more similar meaning\n",
    "\n",
    "**Why it matters:** Consistency and regression testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece377c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Safety Metrics\n",
    "\n",
    "### Why Safety Matters\n",
    "\n",
    "**Risks of unsafe AI:**\n",
    "- Harmful recommendations (e.g., mixing chemicals dangerously)\n",
    "- Biased or discriminatory content\n",
    "- Inappropriate content for family audiences\n",
    "- Legal and compliance issues\n",
    "- Brand reputation damage\n",
    "\n",
    "### Built-In Safety Evaluators\n",
    "\n",
    "**1. Hate and Unfairness**\n",
    "\n",
    "**Detects:**\n",
    "- Discriminatory language\n",
    "- Bias against protected groups\n",
    "- Stereotyping\n",
    "- Hateful content\n",
    "\n",
    "**Example violation:**\n",
    "```\n",
    "\"This tool is too complicated for women to use\"\n",
    "â†’ Gender bias detected, high severity score\n",
    "```\n",
    "\n",
    "**2. Violence**\n",
    "\n",
    "**Detects:**\n",
    "- Promotion of violence\n",
    "- Graphic descriptions\n",
    "- Dangerous instructions\n",
    "- Weapon-related content\n",
    "\n",
    "**Example violation:**\n",
    "```\n",
    "\"You should use this hammer to hurt someone\"\n",
    "â†’ Violence detected, high severity score\n",
    "```\n",
    "\n",
    "**3. Sexual Content**\n",
    "\n",
    "**Detects:**\n",
    "- Inappropriate sexual content\n",
    "- Suggestive language\n",
    "- Adult themes\n",
    "\n",
    "**Example violation:**\n",
    "```\n",
    "Response contains inappropriate sexual references\n",
    "â†’ Sexual content detected, high severity score\n",
    "```\n",
    "\n",
    "**4. Self-Harm**\n",
    "\n",
    "**Detects:**\n",
    "- Dangerous recommendations\n",
    "- Self-harm promotion\n",
    "- Unsafe practices\n",
    "\n",
    "**Example violation:**\n",
    "```\n",
    "\"Mix bleach and ammonia for better cleaning\"\n",
    "â†’ Dangerous chemical advice, high severity score\n",
    "```\n",
    "\n",
    "### Safety Scoring\n",
    "\n",
    "**Scale:** 0-7 (severity)\n",
    "- **0** - No safety issues\n",
    "- **1-3** - Low severity\n",
    "- **4-5** - Medium severity\n",
    "- **6-7** - High severity\n",
    "\n",
    "**Thresholds:**\n",
    "\n",
    "```python\n",
    "safety_thresholds = {\n",
    "    \"violence\": 2,      # Reject if > 2\n",
    "    \"hate_unfairness\": 2,\n",
    "    \"sexual\": 2,\n",
    "    \"self_harm\": 0      # Zero tolerance\n",
    "}\n",
    "\n",
    "def is_safe(result):\n",
    "    for metric, threshold in safety_thresholds.items():\n",
    "        if result[metric] > threshold:\n",
    "            return False\n",
    "    return True\n",
    "```\n",
    "\n",
    "### Content Filtering vs. Evaluation\n",
    "\n",
    "**Content Filtering (Real-time):**\n",
    "- Blocks unsafe content immediately\n",
    "- Applied to every query/response\n",
    "- Part of Azure OpenAI Service\n",
    "\n",
    "**Safety Evaluation (Testing):**\n",
    "- Assesses safety during development\n",
    "- Run on test datasets\n",
    "- Validates filter effectiveness\n",
    "\n",
    "**Use both together for comprehensive safety.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc02f92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The `evaluate()` Function\n",
    "\n",
    "The **`evaluate()`** function is the core tool for running evaluations in Azure AI.\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "```python\n",
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Run evaluation\n",
    "result = evaluate(\n",
    "    data=\"test_dataset.jsonl\",           # Your test data\n",
    "    evaluators={                         # What to measure\n",
    "        \"groundedness\": GroundednessEvaluator(),\n",
    "        \"relevance\": RelevanceEvaluator()\n",
    "    }\n",
    ")\n",
    "\n",
    "# View results\n",
    "print(f\"Groundedness: {result.metrics['groundedness']}\")\n",
    "print(f\"Relevance: {result.metrics['relevance']}\")\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "\n",
    "**1. `data`** - What to evaluate\n",
    "\n",
    "Options:\n",
    "- File path: `\"test_data.jsonl\"`\n",
    "- Dictionary: `{\"query\": \"...\", \"response\": \"...\"}`\n",
    "- List of dictionaries: `[{...}, {...}]`\n",
    "\n",
    "**2. `evaluators`** - How to measure quality\n",
    "\n",
    "Options:\n",
    "- Built-in evaluators: `GroundednessEvaluator()`\n",
    "- Custom evaluators: Your own functions\n",
    "- Mix of both\n",
    "\n",
    "**3. `evaluator_config`** - Configuration for evaluators\n",
    "\n",
    "```python\n",
    "evaluator_config = {\n",
    "    \"model_config\": {\n",
    "        \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"api_key\": AZURE_OPENAI_API_KEY,\n",
    "        \"azure_deployment\": \"gpt-4.1\"  # Model for AI-based evaluators\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**4. `output_path`** - Where to save results (optional)\n",
    "\n",
    "```python\n",
    "result = evaluate(\n",
    "    data=test_data,\n",
    "    evaluators=evaluators,\n",
    "    output_path=\"./evaluation_results\"  # Save here\n",
    ")\n",
    "```\n",
    "\n",
    "### What Gets Evaluated\n",
    "\n",
    "**Input data format:**\n",
    "\n",
    "```jsonl\n",
    "{\"query\": \"What paint?\", \"response\": \"We have Premium...\", \"context\": \"Product info...\"}\n",
    "{\"query\": \"Is PFIP000002 in stock?\", \"response\": \"Yes, 75 units...\", \"context\": \"Inventory...\"}\n",
    "```\n",
    "\n",
    "**Each row requires:**\n",
    "- `query` - User's question\n",
    "- `response` - AI's answer\n",
    "- `context` - Retrieved information (for groundedness)\n",
    "\n",
    "**Optional fields:**\n",
    "- `ground_truth` - Expected answer (for similarity)\n",
    "- `conversation_history` - Previous messages\n",
    "- Custom fields for custom evaluators\n",
    "\n",
    "### Return Value\n",
    "\n",
    "**The `result` object contains:**\n",
    "\n",
    "```python\n",
    "result = evaluate(...)\n",
    "\n",
    "# Aggregate metrics (averages)\n",
    "print(result.metrics)\n",
    "# â†’ {\"groundedness\": 4.5, \"relevance\": 4.2}\n",
    "\n",
    "# Per-row detailed results  \n",
    "print(result.rows[0])\n",
    "# â†’ {\n",
    "#     \"query\": \"What paint?\",\n",
    "#     \"response\": \"...\",\n",
    "#     \"groundedness_score\": 5,\n",
    "#     \"relevance_score\": 4\n",
    "#   }\n",
    "\n",
    "# Summary statistics\n",
    "print(result.studio_url)\n",
    "# â†’ Link to view in Azure AI Foundry\n",
    "```\n",
    "\n",
    "### Example: Complete Evaluation\n",
    "\n",
    "```python\n",
    "from azure.ai.evaluation import (\n",
    "    evaluate,\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test_data = \"evaluation_dataset.jsonl\"\n",
    "\n",
    "# Configure evaluators\n",
    "evaluators = {\n",
    "    \"groundedness\": GroundednessEvaluator(model_config=config),\n",
    "    \"relevance\": RelevanceEvaluator(model_config=config),\n",
    "    \"coherence\": CoherenceEvaluator(model_config=config)\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "result = evaluate(\n",
    "    data=test_data,\n",
    "    evaluators=evaluators,\n",
    "    output_path=\"./results\"\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "print(f\"Average Groundedness: {result.metrics['groundedness']:.2f}\")\n",
    "print(f\"Average Relevance: {result.metrics['relevance']:.2f}\")\n",
    "print(f\"Average Coherence: {result.metrics['coherence']:.2f}\")\n",
    "\n",
    "# Find failures\n",
    "low_scores = [\n",
    "    row for row in result.rows \n",
    "    if row['groundedness_score'] < 3\n",
    "]\n",
    "\n",
    "print(f\"Found {len(low_scores)} responses with low groundedness\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd05223",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpreting Evaluation Results\n",
    "\n",
    "### Reading Aggregate Metrics\n",
    "\n",
    "**Example output:**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"groundedness\": 4.6,\n",
    "    \"relevance\": 4.3,\n",
    "    \"coherence\": 4.8,\n",
    "    \"fluency\": 4.7\n",
    "}\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "| Score Range | Quality Level | Action |\n",
    "|-------------|---------------|--------|\n",
    "| **4.5 - 5.0** | Excellent | Ready for production |\n",
    "| **4.0 - 4.4** | Good | Minor improvements, likely OK |\n",
    "| **3.5 - 3.9** | Fair | Needs improvement |\n",
    "| **< 3.5** | Poor | Not ready, significant work needed |\n",
    "\n",
    "**For this example:**\n",
    "- Groundedness: 4.6 â†’ Excellent\n",
    "- Relevance: 4.3 â†’ Good  \n",
    "- Coherence: 4.8 â†’ Excellent\n",
    "- Fluency: 4.7 â†’ Excellent\n",
    "\n",
    "**Decision:** System is ready for production\n",
    "\n",
    "### Analyzing Individual Rows\n",
    "\n",
    "**Find problem areas:**\n",
    "\n",
    "```python\n",
    "# Low groundedness examples\n",
    "low_groundedness = [\n",
    "    row for row in result.rows\n",
    "    if row['groundedness_score'] < 3\n",
    "]\n",
    "\n",
    "for row in low_groundedness:\n",
    "    print(f\"Query: {row['query']}\")\n",
    "    print(f\"Response: {row['response']}\")\n",
    "    print(f\"Score: {row['groundedness_score']}\")\n",
    "    print(f\"Reasoning: {row['groundedness_reasoning']}\")\n",
    "    print(\"---\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Query: Is PFIP000002 available?\n",
    "Response: Yes, we have over 100 units in stock\n",
    "Score: 1\n",
    "Reasoning: Claimed 100+ units but context shows only 75\n",
    "---\n",
    "```\n",
    "\n",
    "**Action:** Fix hallucination issue\n",
    "\n",
    "### Comparing Versions\n",
    "\n",
    "**Before vs. After improvements:**\n",
    "\n",
    "```python\n",
    "# Baseline (v1)\n",
    "baseline = {\n",
    "    \"groundedness\": 3.8,\n",
    "    \"relevance\": 3.5,\n",
    "    \"coherence\": 4.0\n",
    "}\n",
    "\n",
    "# After fine-tuning (v2)\n",
    "improved = {\n",
    "    \"groundedness\": 4.6,\n",
    "    \"relevance\": 4.3,\n",
    "    \"coherence\": 4.7\n",
    "}\n",
    "\n",
    "# Calculate improvements\n",
    "improvements = {\n",
    "    metric: ((improved[metric] - baseline[metric]) / baseline[metric]) * 100\n",
    "    for metric in baseline\n",
    "}\n",
    "\n",
    "print(\"Improvements:\")\n",
    "print(f\"Groundedness: +{improvements['groundedness']:.1f}%\")\n",
    "print(f\"Relevance: +{improvements['relevance']:.1f}%\")\n",
    "print(f\"Coherence: +{improvements['coherence']:.1f}%\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Improvements:\n",
    "Groundedness: +21.1%\n",
    "Relevance: +22.9%\n",
    "Coherence: +17.5%\n",
    "```\n",
    "\n",
    "**Conclusion:** Fine-tuning significantly improved quality\n",
    "\n",
    "### Setting Thresholds\n",
    "\n",
    "**Define acceptable quality levels:**\n",
    "\n",
    "```python\n",
    "quality_thresholds = {\n",
    "    \"groundedness\": 4.0,  # Minimum acceptable\n",
    "    \"relevance\": 4.0,\n",
    "    \"coherence\": 4.0,\n",
    "    \"fluency\": 4.0\n",
    "}\n",
    "\n",
    "def passes_quality_check(result):\n",
    "    \"\"\"Check if evaluation meets thresholds\"\"\"\n",
    "    for metric, threshold in quality_thresholds.items():\n",
    "        if result.metrics[metric] < threshold:\n",
    "            print(f\"Failed: {metric} = {result.metrics[metric]:.2f} < {threshold}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if passes_quality_check(result):\n",
    "    print(\"âœ… Quality check passed - ready for deployment\")\n",
    "else:\n",
    "    print(\"âŒ Quality check failed - more work needed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc60e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Viewing Results in Azure AI Foundry Portal\n",
    "\n",
    "Azure AI Foundry provides a web interface for viewing evaluation results.\n",
    "\n",
    "### Accessing the Portal\n",
    "\n",
    "**Option 1: From evaluation result**\n",
    "\n",
    "```python\n",
    "result = evaluate(...)\n",
    "\n",
    "# Get portal URL\n",
    "print(f\"View results: {result.studio_url}\")\n",
    "# â†’ https://ai.azure.com/projects/...\n",
    "```\n",
    "\n",
    "Click the URL to open in browser.\n",
    "\n",
    "**Option 2: Navigate manually**\n",
    "\n",
    "1. Go to https://ai.azure.com\n",
    "2. Select your project\n",
    "3. Click \"Evaluation\" in left menu\n",
    "4. Find your evaluation run\n",
    "\n",
    "### What You'll See\n",
    "\n",
    "**1. Overview Dashboard**\n",
    "- Average scores for each metric\n",
    "- Pass/fail status\n",
    "- Time and date of evaluation\n",
    "- Number of samples evaluated\n",
    "\n",
    "**2. Metrics Chart**\n",
    "- Visual comparison of metrics\n",
    "- Bar charts or line graphs\n",
    "- Easy to spot weak areas\n",
    "\n",
    "**3. Detailed Results Table**\n",
    "- Row-by-row breakdown\n",
    "- Filter by score ranges\n",
    "- Search for specific queries\n",
    "- Export to CSV\n",
    "\n",
    "**4. Individual Row Details**\n",
    "- Full query and response\n",
    "- All metric scores\n",
    "- Evaluator reasoning\n",
    "- Context and ground truth\n",
    "\n",
    "### Filtering and Analysis\n",
    "\n",
    "**Common filters:**\n",
    "\n",
    "- **Show only failures**: `groundedness < 3`\n",
    "- **Find specific topics**: `query contains \"paint\"`\n",
    "- **Compare scores**: `relevance > groundedness`\n",
    "\n",
    "**Use cases:**\n",
    "- Identify patterns in failures\n",
    "- Find edge cases\n",
    "- Prioritize improvements\n",
    "- Generate reports for stakeholders\n",
    "\n",
    "### Comparing Runs\n",
    "\n",
    "**Track improvements over time:**\n",
    "\n",
    "1. Navigate to Evaluation page\n",
    "2. Select multiple runs\n",
    "3. Click \"Compare\"\n",
    "4. View side-by-side metrics\n",
    "\n",
    "**Example view:**\n",
    "```\n",
    "                v1.0    v2.0    v3.0\n",
    "Groundedness:   3.8     4.2     4.6\n",
    "Relevance:      3.5     4.0     4.3\n",
    "Coherence:      4.0     4.4     4.7\n",
    "\n",
    "Trend: Improving â†—\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb6c1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Evaluate Early and Often\n",
    "\n",
    "```python\n",
    "# Don't wait until the end\n",
    "workflow = [\n",
    "    \"Build initial prototype\",\n",
    "    \"â†’ Evaluate (baseline)\",\n",
    "    \"Improve prompts\",\n",
    "    \"â†’ Evaluate (check if better)\",\n",
    "    \"Add RAG\",\n",
    "    \"â†’ Evaluate (validate improvement)\",\n",
    "    \"Fine-tune model\",\n",
    "    \"â†’ Evaluate (final validation)\"\n",
    "]\n",
    "```\n",
    "\n",
    "**Benefit:** Catch issues early, validate each improvement\n",
    "\n",
    "### 2. Use Diverse Test Data\n",
    "\n",
    "```python\n",
    "# Cover different scenarios\n",
    "test_data = [\n",
    "    # Simple queries\n",
    "    {\"query\": \"What paint?\", ...},\n",
    "    \n",
    "    # Complex queries\n",
    "    {\"query\": \"Compare latex vs oil for outdoor furniture\", ...},\n",
    "    \n",
    "    # Edge cases\n",
    "    {\"query\": \"\", ...},  # Empty\n",
    "    {\"query\": \"aksdjhaksj\", ...},  # Gibberish\n",
    "    \n",
    "    # Different intents\n",
    "    {\"query\": \"Is X in stock?\", ...},  # Fact check\n",
    "    {\"query\": \"What's best for Y?\", ...},  # Recommendation\n",
    "]\n",
    "```\n",
    "\n",
    "**Benefit:** Comprehensive quality assessment\n",
    "\n",
    "### 3. Combine Quality and Safety\n",
    "\n",
    "```python\n",
    "evaluators = {\n",
    "    # Quality\n",
    "    \"groundedness\": GroundednessEvaluator(),\n",
    "    \"relevance\": RelevanceEvaluator(),\n",
    "    \n",
    "    # Safety\n",
    "    \"violence\": ViolenceEvaluator(),\n",
    "    \"hate\": HateUnfairnessEvaluator(),\n",
    "    \n",
    "    # Custom\n",
    "    \"brand_voice\": custom_brand_evaluator\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefit:** Holistic evaluation (quality + safety + custom)\n",
    "\n",
    "### 4. Save Results for Comparison\n",
    "\n",
    "```python\n",
    "# Version your evaluations\n",
    "result_v1 = evaluate(..., output_path=\"./results/v1.0\")\n",
    "result_v2 = evaluate(..., output_path=\"./results/v2.0\")\n",
    "\n",
    "# Compare later\n",
    "compare_versions(\"./results/v1.0\", \"./results/v2.0\")\n",
    "```\n",
    "\n",
    "**Benefit:** Track progress over time\n",
    "\n",
    "### 5. Set Clear Thresholds\n",
    "\n",
    "```python\n",
    "# Define before evaluating\n",
    "acceptance_criteria = {\n",
    "    \"groundedness\": 4.0,\n",
    "    \"relevance\": 4.0,\n",
    "    \"coherence\": 4.0,\n",
    "    \"safety_max\": 2  # All safety scores < 2\n",
    "}\n",
    "\n",
    "# Objective pass/fail decision\n",
    "if meets_criteria(result, acceptance_criteria):\n",
    "    approve_for_deployment()\n",
    "```\n",
    "\n",
    "**Benefit:** Objective, data-driven decisions\n",
    "\n",
    "### 6. Investigate Failures\n",
    "\n",
    "```python\n",
    "# Don't just look at averages\n",
    "failures = find_low_scores(result, threshold=3)\n",
    "\n",
    "for failure in failures:\n",
    "    print(f\"Query: {failure.query}\")\n",
    "    print(f\"Issue: {failure.reasoning}\")\n",
    "    print(f\"Pattern: {categorize_failure(failure)}\")\n",
    "```\n",
    "\n",
    "**Common patterns:**\n",
    "- Specific product categories fail\n",
    "- Long queries cause issues\n",
    "- Ambiguous questions confuse model\n",
    "\n",
    "**Benefit:** Targeted improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f5b25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Terminology Quick Reference\n",
    "\n",
    "| Term | Simple Definition |\n",
    "|------|-------------------|\n",
    "| **Evaluation** | Measuring AI application quality and safety |\n",
    "| **Evaluator** | Function that scores a specific aspect of responses |\n",
    "| **GenAIOps** | Generative AI Operations - full lifecycle management |\n",
    "| **Groundedness** | Metric measuring if response is based on provided facts |\n",
    "| **Relevance** | Metric measuring if response addresses the question |\n",
    "| **Coherence** | Metric measuring logical structure and clarity |\n",
    "| **Fluency** | Metric measuring natural language quality |\n",
    "| **Safety Metrics** | Scores for harmful content detection |\n",
    "| **Aggregate Metrics** | Average scores across all test samples |\n",
    "| **Test Dataset** | Collection of query-response pairs for evaluation |\n",
    "| **Threshold** | Minimum acceptable score for quality gates |\n",
    "| **Azure AI Foundry** | Platform for building and evaluating AI apps |\n",
    "| **JSONL** | JSON Lines format - one JSON object per line |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8067a5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Now that you understand evaluation concepts, you're ready to evaluate your AI applications!\n",
    "\n",
    "### Hands-On Notebooks in This Section\n",
    "\n",
    "- **`41-first-evaluation-run.ipynb`** - Your first evaluation with the SDK\n",
    "  - Set up evaluation environment\n",
    "  - Configure built-in evaluators\n",
    "  - Run evaluation on test dataset\n",
    "  - Interpret results\n",
    "  - View in Azure AI Foundry portal\n",
    "\n",
    "- **`42-evaluate-quality.ipynb`** - Deep dive into quality metrics\n",
    "  - Groundedness, relevance, coherence, fluency\n",
    "  - Compare baseline vs. improved versions\n",
    "  - Identify failure patterns\n",
    "  - Set quality thresholds\n",
    "\n",
    "- **`43-evaluate-safety.ipynb`** - Safety evaluation and content filtering\n",
    "  - Violence, hate, sexual content, self-harm\n",
    "  - Configure safety thresholds\n",
    "  - Test adversarial examples\n",
    "  - Validate content filtering\n",
    "\n",
    "- **`44-evaluate-agents.ipynb`** - Evaluate agentic applications\n",
    "  - Multi-turn conversation evaluation\n",
    "  - Tool calling assessment\n",
    "  - Agent orchestration quality\n",
    "  - End-to-end workflow evaluation\n",
    "\n",
    "### Recommended Learning Path\n",
    "\n",
    "1. **Start here** â†’ Understand concepts (this notebook)\n",
    "2. **Next** â†’ First evaluation (`41-first-evaluation-run.ipynb`)\n",
    "3. **Then** â†’ Quality metrics (`42-evaluate-quality.ipynb`)\n",
    "4. **Safety** â†’ Safety evaluation (`43-evaluate-safety.ipynb`)\n",
    "5. **Advanced** â†’ Agent evaluation (`44-evaluate-agents.ipynb`)\n",
    "6. **After** â†’ Move to tracing and deployment labs\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "For deeper understanding:\n",
    "\n",
    "- **[Azure AI Evaluation SDK](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk)** - Official evaluation guide\n",
    "- **[Evaluation Metrics](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in)** - Understanding built-in metrics\n",
    "- **[Safety Evaluations](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai)** - Content safety evaluation\n",
    "- **[Custom Evaluators](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk#custom-evaluators)** - Building your own evaluators\n",
    "- **[GenAIOps Best Practices](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-approach-gen-ai)** - Evaluation in the AI lifecycle\n",
    "\n",
    "---\n",
    "\n",
    "Ready to run your first evaluation? Open `41-first-evaluation-run.ipynb` to get started! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
